{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\py36\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import ast\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "# Linear Models \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>listOfCleanWords</th>\n",
       "      <th>cleanWordsAsText</th>\n",
       "      <th>BagOfWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['explanation', 'edits', 'made', 'username', '...</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['aww', 'match', 'background', 'colour', 'seem...</td>\n",
       "      <td>aww match background colour seemingly stuck th...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['hey', 'man', 'really', 'trying', 'edit', 'wa...</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['make', 'real', 'suggestion', 'improvement', ...</td>\n",
       "      <td>make real suggestion improvement wondered sect...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['sir', 'hero', 'chance', 'remember', 'page']</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>00025465d4725e87</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['congratulation', 'well', 'use', 'tool', 'wel...</td>\n",
       "      <td>congratulation well use tool well talk</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['cocksucker', 'piss', 'around', 'work']</td>\n",
       "      <td>cocksucker piss around work</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>00031b1e95af7921</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['vandalism', 'matt', 'shirvington', 'article'...</td>\n",
       "      <td>vandalism matt shirvington article reverted pl...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>00037261f536c51d</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['sorry', 'word', 'nonsense', 'offensive', 'an...</td>\n",
       "      <td>sorry word nonsense offensive anyway intending...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>00040093b2687caa</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['alignment', 'subject', 'contrary', 'dulithgow']</td>\n",
       "      <td>alignment subject contrary dulithgow</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                id  \\\n",
       "0           0  0000997932d777bf   \n",
       "1           1  000103f0d9cfb60f   \n",
       "2           2  000113f07ec002fd   \n",
       "3           3  0001b41b1c6bb37e   \n",
       "4           4  0001d958c54c6e35   \n",
       "5           5  00025465d4725e87   \n",
       "6           6  0002bcb3da6cb337   \n",
       "7           7  00031b1e95af7921   \n",
       "8           8  00037261f536c51d   \n",
       "9           9  00040093b2687caa   \n",
       "\n",
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0             0   \n",
       "1  D'aww! He matches this background colour I'm s...      0             0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0             0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0             0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0             0   \n",
       "5  \"\\n\\nCongratulations from me as well, use the ...      0             0   \n",
       "6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1             1   \n",
       "7  Your vandalism to the Matt Shirvington article...      0             0   \n",
       "8  Sorry if the word 'nonsense' was offensive to ...      0             0   \n",
       "9  alignment on this subject and which are contra...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  \\\n",
       "0        0       0       0              0   \n",
       "1        0       0       0              0   \n",
       "2        0       0       0              0   \n",
       "3        0       0       0              0   \n",
       "4        0       0       0              0   \n",
       "5        0       0       0              0   \n",
       "6        1       0       1              0   \n",
       "7        0       0       0              0   \n",
       "8        0       0       0              0   \n",
       "9        0       0       0              0   \n",
       "\n",
       "                                    listOfCleanWords  \\\n",
       "0  ['explanation', 'edits', 'made', 'username', '...   \n",
       "1  ['aww', 'match', 'background', 'colour', 'seem...   \n",
       "2  ['hey', 'man', 'really', 'trying', 'edit', 'wa...   \n",
       "3  ['make', 'real', 'suggestion', 'improvement', ...   \n",
       "4      ['sir', 'hero', 'chance', 'remember', 'page']   \n",
       "5  ['congratulation', 'well', 'use', 'tool', 'wel...   \n",
       "6           ['cocksucker', 'piss', 'around', 'work']   \n",
       "7  ['vandalism', 'matt', 'shirvington', 'article'...   \n",
       "8  ['sorry', 'word', 'nonsense', 'offensive', 'an...   \n",
       "9  ['alignment', 'subject', 'contrary', 'dulithgow']   \n",
       "\n",
       "                                    cleanWordsAsText      BagOfWords  \n",
       "0  explanation edits made username hardcore metal...  <class 'dict'>  \n",
       "1  aww match background colour seemingly stuck th...  <class 'dict'>  \n",
       "2  hey man really trying edit war guy constantly ...  <class 'dict'>  \n",
       "3  make real suggestion improvement wondered sect...  <class 'dict'>  \n",
       "4                      sir hero chance remember page  <class 'dict'>  \n",
       "5             congratulation well use tool well talk  <class 'dict'>  \n",
       "6                        cocksucker piss around work  <class 'dict'>  \n",
       "7  vandalism matt shirvington article reverted pl...  <class 'dict'>  \n",
       "8  sorry word nonsense offensive anyway intending...  <class 'dict'>  \n",
       "9               alignment subject contrary dulithgow  <class 'dict'>  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nameTrainCSV = 'trainWithListOfCleanWords'\n",
    "nameTestCSV = 'testWithListOfCleanWords'\n",
    "\n",
    "train = pd.read_csv('../data/processed/' + nameTrainCSV + '.csv', encoding='utf-8')\n",
    "train['BagOfWords'] = dict\n",
    "train.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.68869686126709\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for x in range(len(train)):\n",
    "    train.set_value(col='listOfCleanWords',\n",
    "                index=x,\n",
    "                value=ast.literal_eval(train[\"listOfCleanWords\"][x]))\n",
    "    train.set_value(col='BagOfWords',\n",
    "                index=x,\n",
    "                value=Counter(train[\"listOfCleanWords\"][x]))\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>listOfCleanWords</th>\n",
       "      <th>cleanWordsAsText</th>\n",
       "      <th>BagOfWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[explanation, edits, made, username, hardcore,...</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>{'explanation': 1, 'edits': 1, 'made': 1, 'use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[aww, match, background, colour, seemingly, st...</td>\n",
       "      <td>aww match background colour seemingly stuck th...</td>\n",
       "      <td>{'aww': 1, 'match': 1, 'background': 1, 'colou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[hey, man, really, trying, edit, war, guy, con...</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>{'hey': 1, 'man': 1, 'really': 1, 'trying': 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[make, real, suggestion, improvement, wondered...</td>\n",
       "      <td>make real suggestion improvement wondered sect...</td>\n",
       "      <td>{'make': 1, 'real': 1, 'suggestion': 1, 'impro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[sir, hero, chance, remember, page]</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>{'sir': 1, 'hero': 1, 'chance': 1, 'remember':...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                id  \\\n",
       "0           0  0000997932d777bf   \n",
       "1           1  000103f0d9cfb60f   \n",
       "2           2  000113f07ec002fd   \n",
       "3           3  0001b41b1c6bb37e   \n",
       "4           4  0001d958c54c6e35   \n",
       "\n",
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0             0   \n",
       "1  D'aww! He matches this background colour I'm s...      0             0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0             0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0             0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  \\\n",
       "0        0       0       0              0   \n",
       "1        0       0       0              0   \n",
       "2        0       0       0              0   \n",
       "3        0       0       0              0   \n",
       "4        0       0       0              0   \n",
       "\n",
       "                                    listOfCleanWords  \\\n",
       "0  [explanation, edits, made, username, hardcore,...   \n",
       "1  [aww, match, background, colour, seemingly, st...   \n",
       "2  [hey, man, really, trying, edit, war, guy, con...   \n",
       "3  [make, real, suggestion, improvement, wondered...   \n",
       "4                [sir, hero, chance, remember, page]   \n",
       "\n",
       "                                    cleanWordsAsText  \\\n",
       "0  explanation edits made username hardcore metal...   \n",
       "1  aww match background colour seemingly stuck th...   \n",
       "2  hey man really trying edit war guy constantly ...   \n",
       "3  make real suggestion improvement wondered sect...   \n",
       "4                      sir hero chance remember page   \n",
       "\n",
       "                                          BagOfWords  \n",
       "0  {'explanation': 1, 'edits': 1, 'made': 1, 'use...  \n",
       "1  {'aww': 1, 'match': 1, 'background': 1, 'colou...  \n",
       "2  {'hey': 1, 'man': 1, 'really': 1, 'trying': 1,...  \n",
       "3  {'make': 1, 'real': 1, 'suggestion': 1, 'impro...  \n",
       "4  {'sir': 1, 'hero': 1, 'chance': 1, 'remember':...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MULTICLASS PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classLabel = {\n",
    "    0: \"neutral\",\n",
    "    1: \"toxic\",\n",
    "    2 : \"severe_toxic\",\n",
    "    3 : \"obscene\",\n",
    "    4 : \"threat\",\n",
    "    5 : \"insult\",\n",
    "    6 : \"identity_hate\" \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.empty((len(train['cleanWordsAsText']),),dtype=object)\n",
    "allTextToxicTrain = dict()\n",
    "for idx in classLabel:\n",
    "    if classLabel[idx] != \"neutral\":\n",
    "        T = np.where(train[classLabel[idx]] == 1)[0]\n",
    "        allTextToxicTrain[idx] = T\n",
    "        for i in T:\n",
    "            if y[i] is None:\n",
    "                y[i] = [idx]                \n",
    "            else:\n",
    "                y[i].append(idx)\n",
    "indxsOfNeutralTexts = np.where(y == None) \n",
    "y[indxsOfNeutralTexts] = [[0]]\n",
    "indxsOfNeutralTexts = indxsOfNeutralTexts[0]\n",
    "\n",
    "allTextsNoToxicTrain = [str(train['cleanWordsAsText'][x]) for x in indxsOfNeutralTexts]\n",
    "\n",
    "idxList = []\n",
    "for i in allTextToxicTrain.keys():\n",
    "    #allTextToxicTrain[i] = [str(train['cleanWordsAsText'][j]) for j in allTextToxicTrain[i]]\n",
    "    idxList = np.unique(np.append(idxList, allTextToxicTrain[i]))\n",
    "allTextToxicTrain = [str(train['cleanWordsAsText'][j]) for j in idxList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = [\"idExp\",\"numFeatures\", \"algorithm\", \"Nfolds\", \"accuaracy\", \"logloss\", \"fmeasure\"]\n",
    "dfTestResults = pd.DataFrame(columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listTopNFeatures = [10,50,100,200,500,1000]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "listTopNFeatures = [10,50,100,200,500,1000]\n",
    "idExp = 0\n",
    "\n",
    "for topNFeatures in listTopNFeatures:\n",
    "    Nfolds = 5\n",
    "    ###### GET THE BEST WORDS IN ALL CORPUS #####\n",
    "    #Neutral data\n",
    "\n",
    "    tdidfVectTrainNoToxic = TfidfVectorizer(max_features=topNFeatures)\n",
    "    matrixTdidfTrainNoToxic = tdidfVectTrainNoToxic.fit_transform(allTextsNoToxicTrain)\n",
    "\n",
    "    featureArrayNoToxicTrain = np.array(tdidfVectTrainNoToxic.get_feature_names())\n",
    "    tfidfOrderedNoToxicTrain = np.argsort(matrixTdidfTrainNoToxic.toarray()).flatten()[::-1]\n",
    "    selectedWordsTrainNoToxic = featureArrayNoToxicTrain[tfidfOrderedNoToxicTrain][:topNFeatures]\n",
    "\n",
    "    #Toxic data\n",
    "    selectedWordsTrainToxic = []\n",
    "    selectedWordsTrain = []\n",
    "\n",
    "    tdidfVectTrainToxic = TfidfVectorizer(max_features=topNFeatures)\n",
    "    matrixTdidfTrainToxic = tdidfVectTrainToxic.fit_transform(allTextToxicTrain)\n",
    "    featureArrayToxicTrain = np.array(tdidfVectTrainToxic.get_feature_names())\n",
    "    tfidfOrderedToxicTrain = np.argsort(matrixTdidfTrainToxic.toarray()).flatten()[::-1]\n",
    "    selectedWordsTrainToxic = featureArrayToxicTrain[tfidfOrderedToxicTrain][:topNFeatures]\n",
    "\n",
    "    # Get words\n",
    "    for x in range(topNFeatures):\n",
    "        if len(selectedWordsTrain) <= topNFeatures: \n",
    "            words = [selectedWordsTrainToxic[x]] + [selectedWordsTrainNoToxic[x]]\n",
    "            selectedWordsTrain = np.unique(np.append(selectedWordsTrain, np.unique(words)))    \n",
    "    selectedWordsTrain = selectedWordsTrain[:topNFeatures]\n",
    "\n",
    "    allTrainText = [txt if txt is not np.nan else '' for txt in train['cleanWordsAsText']]\n",
    "\n",
    "    X = allTrainText\n",
    "    yBinary = MultiLabelBinarizer().fit_transform(y)\n",
    "\n",
    "    kf = KFold(n_splits=Nfolds, random_state=True)\n",
    "    kf.get_n_splits(allTrainText)\n",
    "    \n",
    "    print(str(topNFeatures))\n",
    "    #################################################################\n",
    "    # PREDICCION MULTICLASS Linear SVC\n",
    "\n",
    "    meanAcc = 0.0\n",
    "    meanLogLoss = 0.0\n",
    "    meanFmeasure = 0.0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train = [X[i] for i in train_index]\n",
    "        X_test = [X[i] for i in test_index]\n",
    "        y_train, y_test = yBinary[train_index], yBinary[test_index]\n",
    "\n",
    "        classifier = Pipeline([\n",
    "        ('vectorizer', CountVectorizer(vocabulary=selectedWordsTrain)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', OneVsRestClassifier(LinearSVC()))])\n",
    "\n",
    "        classifier.fit(X_train, y_train)\n",
    "        predicted = classifier.predict(X_test)\n",
    "        acc = accuracy_score(y_test, predicted)\n",
    "        fmeausre = f1_score(y_test, predicted, labels=[0,1,2,3,4,5,6], average=None)\n",
    "        logloss = log_loss(y_pred=predicted, y_true=y_test)\n",
    "        meanAcc += acc\n",
    "        meanLogLoss += logloss\n",
    "        meanFmeasure += fmeausre\n",
    "    meanAcc = meanAcc / Nfolds\n",
    "    meanLogLoss = meanLogLoss / Nfolds\n",
    "    meanFmeasure = meanFmeasure / Nfolds\n",
    "    dfTestResults.loc[idExp] = [idExp,topNFeatures,\"Linear SVC\",Nfolds,meanAcc,meanLogLoss,meanFmeasure]\n",
    "    print(str(idExp))\n",
    "    idExp += 1\n",
    "\n",
    "    #################################################################\n",
    "    # PREDICCION MULTICLASS Decision Tree Classifier\n",
    "    meanAcc = 0.0\n",
    "    meanLogLoss = 0.0\n",
    "    meanFmeasure = 0.0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train = [X[i] for i in train_index]\n",
    "        X_test = [X[i] for i in test_index]\n",
    "        y_train, y_test = yBinary[train_index], yBinary[test_index]\n",
    "\n",
    "        classifier = Pipeline([\n",
    "        ('vectorizer', CountVectorizer(vocabulary=selectedWordsTrain)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', OneVsRestClassifier(DecisionTreeClassifier()))])\n",
    "\n",
    "        classifier.fit(X_train, y_train)\n",
    "        predicted = classifier.predict(X_test)\n",
    "        acc = accuracy_score(y_test, predicted)\n",
    "        fmeausre = f1_score(y_test, predicted, labels=[0,1,2,3,4,5,6], average=None)\n",
    "        logloss = log_loss(y_pred=predicted, y_true=y_test)\n",
    "        meanAcc += acc\n",
    "        meanLogLoss += logloss\n",
    "        meanFmeasure += fmeausre\n",
    "    meanAcc = meanAcc / Nfolds\n",
    "    meanLogLoss = meanLogLoss / Nfolds\n",
    "    meanFmeasure = meanFmeasure / Nfolds\n",
    "    dfTestResults.loc[idExp] = [idExp,topNFeatures,\"Decision Tree\",Nfolds,meanAcc,meanLogLoss,meanFmeasure]\n",
    "    print(str(idExp))\n",
    "    idExp += 1\n",
    "    #################################################################\n",
    "    # PREDICCION MULTICLASS SGD\n",
    "    meanAcc = 0.0\n",
    "    meanLogLoss = 0.0\n",
    "    meanFmeasure = 0.0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train = [X[i] for i in train_index]\n",
    "        X_test = [X[i] for i in test_index]\n",
    "        y_train, y_test = yBinary[train_index], yBinary[test_index]\n",
    "\n",
    "        classifier = Pipeline([\n",
    "        ('vectorizer', CountVectorizer(vocabulary=selectedWordsTrain)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', OneVsRestClassifier(linear_model.SGDClassifier()))])\n",
    "\n",
    "        classifier.fit(X_train, y_train)\n",
    "        predicted = classifier.predict(X_test)\n",
    "        acc = accuracy_score(y_test, predicted)\n",
    "        fmeausre = f1_score(y_test, predicted, labels=[0,1,2,3,4,5,6], average=None)\n",
    "        logloss = log_loss(y_pred=predicted, y_true=y_test)\n",
    "        meanAcc += acc\n",
    "        meanLogLoss += logloss\n",
    "        meanFmeasure += fmeausre\n",
    "    meanAcc = meanAcc / Nfolds\n",
    "    meanLogLoss = meanLogLoss / Nfolds\n",
    "    meanFmeasure = meanFmeasure / Nfolds\n",
    "    dfTestResults.loc[idExp] = [idExp,topNFeatures,\"SGD\",Nfolds,meanAcc,meanLogLoss,meanFmeasure]\n",
    "    print(str(idExp))\n",
    "    idExp += 1\n",
    "    #################################################################\n",
    "    # PREDICCION MULTICLASS NB\n",
    "    meanAcc = 0.0\n",
    "    meanLogLoss = 0.0\n",
    "    meanFmeasure = 0.0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train = [X[i] for i in train_index]\n",
    "        X_test = [X[i] for i in test_index]\n",
    "        y_train, y_test = yBinary[train_index], yBinary[test_index]\n",
    "\n",
    "        classifier = Pipeline([\n",
    "        ('vectorizer', CountVectorizer(vocabulary=selectedWordsTrain)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', OneVsRestClassifier(MultinomialNB()))])\n",
    "\n",
    "        classifier.fit(X_train, y_train)\n",
    "        predicted = classifier.predict(X_test)\n",
    "        acc = accuracy_score(y_test, predicted)\n",
    "        fmeausre = f1_score(y_test, predicted, labels=[0,1,2,3,4,5,6], average=None)\n",
    "        logloss = log_loss(y_pred=predicted, y_true=y_test)\n",
    "        meanAcc += acc\n",
    "        meanLogLoss += logloss\n",
    "        meanFmeasure += fmeausre\n",
    "    meanAcc = meanAcc / Nfolds\n",
    "    meanLogLoss = meanLogLoss / Nfolds\n",
    "    meanFmeasure = meanFmeasure / Nfolds\n",
    "    dfTestResults.loc[idExp] = [idExp,topNFeatures,\"NB\",Nfolds,meanAcc,meanLogLoss,meanFmeasure]\n",
    "    print(str(idExp))\n",
    "    idExp += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "writer = pd.ExcelWriter('../reports/' + 'results.xlsx')\n",
    "dfTestResults.to_excel(writer,'Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>listOfCleanWords</th>\n",
       "      <th>cleanWordsAsText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "      <td>['yo', 'bitch', 'ja', 'rule', 'succesful', 'ev...</td>\n",
       "      <td>yo bitch ja rule succesful ever whats hating s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "      <td>['rfc', 'title', 'fine', 'imo']</td>\n",
       "      <td>rfc title fine imo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "      <td>['source', 'zawe', 'ashton', 'lapland']</td>\n",
       "      <td>source zawe ashton lapland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "      <td>['look', 'back', 'source', 'information', 'upd...</td>\n",
       "      <td>look back source information updated correct f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "      <td>['anonymously', 'edit', 'article']</td>\n",
       "      <td>anonymously edit article</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                id  \\\n",
       "0           0  00001cee341fdb12   \n",
       "1           1  0000247867823ef7   \n",
       "2           2  00013b17ad220c46   \n",
       "3           3  00017563c3f7919a   \n",
       "4           4  00017695ad8997eb   \n",
       "\n",
       "                                        comment_text  \\\n",
       "0  Yo bitch Ja Rule is more succesful then you'll...   \n",
       "1  == From RfC == \\n\\n The title is fine as it is...   \n",
       "2  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...   \n",
       "3  :If you have a look back at the source, the in...   \n",
       "4          I don't anonymously edit articles at all.   \n",
       "\n",
       "                                    listOfCleanWords  \\\n",
       "0  ['yo', 'bitch', 'ja', 'rule', 'succesful', 'ev...   \n",
       "1                    ['rfc', 'title', 'fine', 'imo']   \n",
       "2            ['source', 'zawe', 'ashton', 'lapland']   \n",
       "3  ['look', 'back', 'source', 'information', 'upd...   \n",
       "4                 ['anonymously', 'edit', 'article']   \n",
       "\n",
       "                                    cleanWordsAsText  \n",
       "0  yo bitch ja rule succesful ever whats hating s...  \n",
       "1                                 rfc title fine imo  \n",
       "2                         source zawe ashton lapland  \n",
       "3  look back source information updated correct f...  \n",
       "4                           anonymously edit article  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test clasification\n",
    "test = pd.read_csv('../data/processed/' + nameTestCSV + '.csv', encoding='utf-8')\n",
    "test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PREDICCION MULTICLASS TEST\n",
    "topNFeatures = listTopNFeatures[3]\n",
    "#Neutral data\n",
    "\n",
    "tdidfVectTrainNoToxic = TfidfVectorizer(max_features=topNFeatures)\n",
    "matrixTdidfTrainNoToxic = tdidfVectTrainNoToxic.fit_transform(allTextsNoToxicTrain)\n",
    "\n",
    "featureArrayNoToxicTrain = np.array(tdidfVectTrainNoToxic.get_feature_names())\n",
    "tfidfOrderedNoToxicTrain = np.argsort(matrixTdidfTrainNoToxic.toarray()).flatten()[::-1]\n",
    "selectedWordsTrainNoToxic = featureArrayNoToxicTrain[tfidfOrderedNoToxicTrain][:topNFeatures]\n",
    "\n",
    "#Toxic data\n",
    "selectedWordsTrainToxic = []\n",
    "selectedWordsTrain = []\n",
    "\n",
    "tdidfVectTrainToxic = TfidfVectorizer(max_features=topNFeatures)\n",
    "matrixTdidfTrainToxic = tdidfVectTrainToxic.fit_transform(allTextToxicTrain)\n",
    "featureArrayToxicTrain = np.array(tdidfVectTrainToxic.get_feature_names())\n",
    "tfidfOrderedToxicTrain = np.argsort(matrixTdidfTrainToxic.toarray()).flatten()[::-1]\n",
    "selectedWordsTrainToxic = featureArrayToxicTrain[tfidfOrderedToxicTrain][:topNFeatures]\n",
    "\n",
    "# Get words\n",
    "for x in range(topNFeatures):\n",
    "    if len(selectedWordsTrain) <= topNFeatures: \n",
    "        words = [selectedWordsTrainToxic[x]] + [selectedWordsTrainNoToxic[x]]\n",
    "        selectedWordsTrain = np.unique(np.append(selectedWordsTrain, np.unique(words)))    \n",
    "selectedWordsTrain = selectedWordsTrain[:topNFeatures]\n",
    "\n",
    "allTrainText = [txt if txt is not np.nan else '' for txt in train['cleanWordsAsText']]\n",
    "allTestText = [txt if txt is not np.nan else '' for txt in test['cleanWordsAsText']]\n",
    "X_train = allTrainText\n",
    "X_test = allTestText\n",
    "yBinary = MultiLabelBinarizer().fit_transform(y)\n",
    "y_train = yBinary\n",
    "\n",
    "\n",
    "vectTrain = CountVectorizer(vocabulary=selectedWordsTrain)\n",
    "X_train_bow = vectTrain.fit_transform(X_train).toarray()\n",
    "X_test_bow = vectTrain.fit_transform(X_test).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "########################### Test Linear SVC\n",
    "\n",
    "classifier = Pipeline([\n",
    "('vectorizer', CountVectorizer(vocabulary=selectedWordsTrain)),\n",
    "('tfidf', TfidfTransformer()),\n",
    "('clf', OneVsRestClassifier(LinearSVC()))])\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "predicted = classifier.predict(X_test)\n",
    "\n",
    "columns = [\"id\",\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "dfTestPredicted = pd.DataFrame(columns=columns)\n",
    "for x in range(len(test)):\n",
    "    dfTestPredicted.loc[x] = [test['id'][x], predicted[x][1], predicted[x][2], predicted[x][3], predicted[x][4], predicted[x][5], predicted[x][6]]\n",
    "\n",
    "dfTestPredicted.to_csv('../reports/testPred/predTestLinear'+ str(topNFeatures) +'.csv',encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "########################### Test Decision Tree\n",
    "\n",
    "classifier = Pipeline([\n",
    "('vectorizer', CountVectorizer(vocabulary=selectedWordsTrain)),\n",
    "('tfidf', TfidfTransformer()),\n",
    "('clf', OneVsRestClassifier(DecisionTreeClassifier()))])\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "predicted = classifier.predict(X_test)\n",
    "\n",
    "columns = [\"id\",\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "dfTestPredicted = pd.DataFrame(columns=columns)\n",
    "for x in range(len(test)):\n",
    "    dfTestPredicted.loc[x] = [test['id'][x], predicted[x][1], predicted[x][2], predicted[x][3], predicted[x][4], predicted[x][5], predicted[x][6]]\n",
    "dfTestPredicted.to_csv('../reports/testPred/predTestDecisionTree'+ str(topNFeatures) +'.csv',encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "########################### Test SGD\n",
    "\n",
    "classifier = Pipeline([\n",
    "('vectorizer', CountVectorizer(vocabulary=selectedWordsTrain)),\n",
    "('tfidf', TfidfTransformer()),\n",
    "('clf', OneVsRestClassifier(linear_model.SGDClassifier()))])\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "predicted = classifier.predict(X_test)\n",
    "\n",
    "columns = [\"id\",\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "dfTestPredicted = pd.DataFrame(columns=columns)\n",
    "for x in range(len(test)):\n",
    "    dfTestPredicted.loc[x] = [test['id'][x], predicted[x][1], predicted[x][2], predicted[x][3], predicted[x][4], predicted[x][5], predicted[x][6]]\n",
    "dfTestPredicted.to_csv('../reports/testPred/predTestSGD'+ str(topNFeatures) +'.csv',encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "########################### Test NB\n",
    "\n",
    "classifier = Pipeline([\n",
    "('vectorizer', CountVectorizer(vocabulary=selectedWordsTrain)),\n",
    "('tfidf', TfidfTransformer()),\n",
    "('clf', OneVsRestClassifier(MultinomialNB()))])\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "predicted = classifier.predict(X_test)\n",
    "\n",
    "columns = [\"id\",\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "dfTestPredicted = pd.DataFrame(columns=columns)\n",
    "for x in range(len(test)):\n",
    "    dfTestPredicted.loc[x] = [test['id'][x], predicted[x][1], predicted[x][2], predicted[x][3], predicted[x][4], predicted[x][5], predicted[x][6]]\n",
    "dfTestPredicted.to_csv('../reports/testPred/predTestNB'+ str(topNFeatures) +'.csv',encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Word 2 Vec mean train list\n",
    "modelWord2VecTrain = models.Word2Vec(train['listOfCleanWords'].tolist(), min_count=1)\n",
    "#say_vector = modelWord2VecTrain['explanation']  # get vector for word\n",
    "#say_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n",
      "C:\\Anaconda2\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "vectorizedTrainW2V = np.empty(len(train['listOfCleanWords']), dtype=list)\n",
    "for i, lstOfCleanWords in enumerate(train['listOfCleanWords']): \n",
    "    size = len(lstOfCleanWords)\n",
    "    if size > 0:\n",
    "        vector = np.zeros(len(modelWord2VecTrain[lstOfCleanWords[0]]))\n",
    "        for x in range(size):\n",
    "            vector += modelWord2VecTrain[lstOfCleanWords[x]]\n",
    "        vectorizedTrainW2V[i] = vector / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ array([ 0.73379291, -0.15204202,  0.16799682, -0.09350665, -0.87342633,\n",
       "       -0.12644328, -0.29345003, -0.47284597,  0.94774098, -0.33401104,\n",
       "       -0.35927565,  1.13237797, -0.07214655,  0.32651917,  0.19017308,\n",
       "        0.17825947, -0.21004198, -0.73069149,  0.64441242, -0.40265178,\n",
       "        1.02520506, -0.47286715,  1.48720414,  0.57169645,  0.42970759,\n",
       "       -0.08890211,  0.63591857,  0.10385049,  0.7023698 , -0.67173565,\n",
       "       -0.74867027,  0.9034874 , -0.13497437, -0.17129793, -0.16749973,\n",
       "       -0.04919034, -0.24335117,  0.13657868,  0.67659103, -0.10070693,\n",
       "       -0.30201212,  0.31276183,  0.22123449, -0.34346243, -0.49338947,\n",
       "       -0.61131901, -0.94420535, -0.19631918,  0.4732139 ,  0.28291244,\n",
       "        0.75220588,  0.44198064,  0.25020587, -0.0982263 ,  0.50892403,\n",
       "        0.17714172,  0.0810034 , -0.16032361,  0.42767846,  0.08251705,\n",
       "        0.4427396 ,  0.27537667, -1.17818363, -0.20494232,  0.48081415,\n",
       "        0.85164736,  0.082337  , -0.19297625,  0.05649744, -0.39879035,\n",
       "        0.46556009, -0.17300864, -0.57014817, -0.63781307,  0.22613907,\n",
       "        0.39144863, -0.03847146,  0.0476001 , -0.42632548,  0.76749723,\n",
       "       -0.32226317,  0.8719316 , -0.84936106,  0.44867489,  0.17652427,\n",
       "        0.18086842,  1.01286234,  0.55291883,  0.14827814, -0.22532763,\n",
       "       -0.81052605, -0.02469582, -0.29539428, -0.36024174,  0.05569193,\n",
       "        0.06887645, -0.7677682 ,  0.02567123,  0.37659557, -0.10351945]),\n",
       "       array([-0.27465632,  0.10528949,  0.20797271,  0.70310061, -0.09085291,\n",
       "        0.34694404,  0.33439046, -0.85654134,  0.58470216, -0.55364172,\n",
       "       -0.95825847, -0.01645686,  0.42103444,  0.28170803,  0.02901151,\n",
       "        0.31974748,  0.07025339, -0.03634632,  0.27259286,  0.17693432,\n",
       "        0.348332  , -0.06725059,  0.99624157,  0.74717848,  0.19284594,\n",
       "       -0.25817905,  0.03703868,  0.09786509,  0.84803202,  0.14044804,\n",
       "       -0.48960889,  0.81005186, -0.31208695,  0.14211719,  0.11525305,\n",
       "       -0.41032544,  0.06224785,  0.25003719,  0.53905236,  0.06106134,\n",
       "       -0.01651445, -0.57254761,  0.48976025, -0.83148806, -0.35918685,\n",
       "       -0.65471012, -0.57775027, -0.53550649,  0.68942224,  0.41455792,\n",
       "        1.23258475,  0.28032512, -0.37174358, -0.10777905,  0.45377111,\n",
       "        0.49300386, -0.08975135, -0.46284334, -0.18583664,  0.18903021,\n",
       "       -0.04028159,  0.22486191, -0.34112155, -0.15433679,  0.061774  ,\n",
       "        0.08840696, -0.2902582 ,  0.26532394,  0.18490065, -0.04121872,\n",
       "        0.43545293, -0.55461769, -0.09131176, -0.33431622, -0.0050166 ,\n",
       "        0.47269755,  0.36102189, -0.19664858, -0.50238243,  0.74431545,\n",
       "       -0.18092842,  0.56215115, -0.18898729,  0.28689848, -0.43536325,\n",
       "       -0.26494889,  0.8060611 ,  0.36603373,  0.75249123, -0.00395128,\n",
       "       -0.34431926,  0.05009636, -0.20445342, -0.30875254,  0.02660267,\n",
       "       -0.57305954, -0.81934424,  0.0355986 , -0.6309792 ,  0.37239581]),\n",
       "       array([-0.37887965, -0.36234914, -0.13809395,  0.24364528, -0.90962047,\n",
       "       -0.09961757, -0.12059778, -0.63442727,  0.4730097 , -0.72687971,\n",
       "       -0.25554645,  1.27084367, -0.9679578 ,  0.41280768,  0.10173538,\n",
       "        0.02946855, -0.50094832, -0.09310964,  1.45953416, -0.41124981,\n",
       "        1.59919377, -0.67442515,  1.15869229,  0.47430322,  0.86366163,\n",
       "       -0.14827687,  0.8766042 ,  0.38138145,  0.59100103, -1.02417222,\n",
       "       -0.61119102,  1.1672465 ,  0.192891  ,  0.16570018,  0.07669236,\n",
       "        0.6841082 , -0.6173485 ,  0.61541003,  0.46354637,  0.42760917,\n",
       "        0.41363017,  0.54416665,  0.25512172, -1.18237694, -0.5770484 ,\n",
       "       -0.99137134, -0.70634707, -0.16707335,  0.60400673,  0.44156425,\n",
       "        0.6268459 ,  0.58295319,  0.22918669,  0.11916621,  0.50724711,\n",
       "        0.48375552, -0.07149027, -0.67020883,  0.3398681 , -0.55938977,\n",
       "        0.6593609 ,  0.75252609, -0.77107109, -0.78128273,  0.35663174,\n",
       "        1.40075386, -0.1621644 , -0.03318616, -0.48657681, -0.35744924,\n",
       "        0.07405856, -0.37179662, -0.71207426, -0.93678808,  0.70115284,\n",
       "        0.27498427, -0.18777626,  0.04868824, -0.85495485,  0.04907103,\n",
       "        0.53305617, -0.04827311, -0.47430238,  0.08633292, -0.80173937,\n",
       "        0.03589184,  0.43903725,  1.01439722,  0.03822042, -0.63050237,\n",
       "       -0.81083491, -0.28249083, -0.71766686, -0.37406461,  0.32119284,\n",
       "       -0.8732474 , -1.03058954, -0.27352156,  0.27405697, -0.00527735]),\n",
       "       ...,\n",
       "       array([-0.03145971, -0.09423645,  0.00267311, -0.00990328, -0.26042123,\n",
       "        0.04463716, -0.27223884,  0.21856469,  0.51946935, -0.29792867,\n",
       "       -0.09752423,  0.27912396,  0.24778414, -0.01071224, -0.10119597,\n",
       "       -0.35146785,  0.07428923, -0.19232192,  0.11856522,  0.16712898,\n",
       "        0.64466972,  0.01056082,  0.55858103,  0.13533956,  0.3907882 ,\n",
       "       -0.01091546,  0.14900654,  0.05777565, -0.07991743, -0.18894145,\n",
       "       -0.0012407 ,  0.10491059, -0.13316996, -0.10158794,  0.05438606,\n",
       "        0.4254106 , -0.20006945, -0.20916731,  0.14439181,  0.23121052,\n",
       "       -0.11592756, -0.36770791,  0.29487966, -0.2541477 , -0.20774534,\n",
       "       -0.01763586,  0.32661958, -0.15708343,  0.4878579 , -0.13612808,\n",
       "        0.31286054,  0.46736318, -0.33179322,  0.04855915,  0.69225418,\n",
       "        0.32207221, -0.13534236, -0.26315479,  0.28236238,  0.23316544,\n",
       "        0.02275272,  0.02101966, -0.39805492, -0.33913259, -0.0166615 ,\n",
       "        0.49709497,  0.21975989,  0.15500697, -0.12015233,  0.21520554,\n",
       "        0.11024832,  0.0020169 , -0.24966831, -0.43025359,  0.33742459,\n",
       "        0.69874827, -0.36390868,  0.07957048, -0.09834463,  0.64657968,\n",
       "        0.33152123,  0.03526293, -0.45003203, -0.08360332, -0.08852319,\n",
       "        0.00853102,  0.45969828,  0.42859942,  0.27702725, -0.24357978,\n",
       "       -0.17592692, -0.04689305, -0.08168488, -0.32254444, -0.34981301,\n",
       "       -0.09455342, -0.0433262 ,  0.19822195,  0.22100658,  0.06330177]),\n",
       "       array([ 0.97374886, -0.23993096, -0.23640914,  0.54327716,  0.33875338,\n",
       "        0.0244791 , -0.54686322,  0.03711662,  0.75422773, -0.94295925,\n",
       "       -0.07565297,  1.4571839 , -0.84923955,  0.06733939, -0.56535669,\n",
       "       -0.09116421,  0.1242934 , -0.50989254,  0.9595022 , -0.08929623,\n",
       "        2.59234758, -1.51678117,  1.96604116,  1.13007406,  0.80196214,\n",
       "       -0.18664805,  0.13106768,  0.8550752 ,  1.46780474, -1.32217484,\n",
       "       -0.28659586,  0.31530527, -0.34624594, -0.25565841,  0.90203621,\n",
       "        0.0684249 , -0.8378282 ,  0.28114374,  0.2707793 ,  0.47278703,\n",
       "       -0.38468738,  0.22442794,  0.93204773, -1.25771021, -1.07371977,\n",
       "       -0.70343177,  0.22382881, -0.59052536, -0.98607815,  0.90364906,\n",
       "        1.45298459,  1.21365374, -0.81733333,  0.08565179,  1.02143825,\n",
       "        1.42511304, -0.24836896,  0.09572525, -0.15860974, -0.31967165,\n",
       "        0.00945551,  0.59970777, -1.77965846,  0.86575295, -0.14200424,\n",
       "        0.38620904, -0.20331835,  0.18679265,  0.56000652,  0.29770011,\n",
       "        0.85804882, -0.18457054, -1.14968214, -0.48069645, -0.77609727,\n",
       "        0.76761688, -0.60438776,  0.51804282, -0.6767716 ,  0.26099655,\n",
       "        0.54745596,  1.22584974, -0.50858653, -0.31274538, -0.51592704,\n",
       "       -1.3340079 ,  0.3697186 ,  1.91011382, -1.30114979, -0.88564912,\n",
       "       -0.65694559, -0.09427103, -0.62542704, -1.23037963, -0.14629406,\n",
       "       -0.79082006, -1.60405793,  0.28353539,  0.05373836, -0.06330158]),\n",
       "       array([ -6.73126035e-01,  -4.89607428e-01,  -5.99819711e-01,\n",
       "         3.05588633e-01,   9.53951711e-02,  -1.12509976e-03,\n",
       "        -3.91511002e-01,  -5.71940309e-01,   9.87376994e-01,\n",
       "        -8.43130389e-01,   6.07272387e-01,   5.49642124e-01,\n",
       "        -1.51257059e+00,   9.90928854e-02,   6.11459072e-01,\n",
       "        -5.85333800e-01,  -3.67536702e-01,   4.06357232e-01,\n",
       "         8.95251876e-01,   3.64509643e-01,   1.75146284e+00,\n",
       "        -4.81452311e-01,   1.29738721e+00,   9.65287737e-01,\n",
       "         1.27007385e+00,   2.60115895e-01,   5.55951202e-01,\n",
       "         6.37075491e-02,   4.26601111e-01,  -8.92624838e-03,\n",
       "        -5.88394127e-01,   1.53592889e+00,  -4.94941854e-02,\n",
       "         7.77436576e-02,   2.79847633e-01,   7.90380079e-01,\n",
       "         3.93750093e-01,   1.84515509e-01,  -7.19147621e-01,\n",
       "         1.23046320e+00,   7.23514767e-01,  -3.38186145e-01,\n",
       "         7.65689184e-01,  -1.19761270e+00,  -5.54928202e-01,\n",
       "        -7.85866544e-01,  -5.85655107e-01,   1.46142073e-01,\n",
       "        -1.65150145e-02,   3.23554297e-01,   6.73689889e-01,\n",
       "         5.02801096e-01,  -8.85515143e-02,  -1.29628875e-01,\n",
       "         8.04184091e-01,   3.94332585e-01,  -4.56643570e-01,\n",
       "        -5.63470264e-01,  -8.32137917e-02,  -6.03980796e-01,\n",
       "         4.93605027e-01,   5.37768870e-01,  -7.73532275e-01,\n",
       "        -7.67607239e-01,   3.85924589e-01,   1.00912870e+00,\n",
       "         3.68519761e-01,  -4.02626259e-01,   9.06248548e-02,\n",
       "        -1.04678808e+00,  -1.95467425e-01,   2.68490161e-02,\n",
       "        -1.47938293e+00,  -8.65798524e-01,   3.29447150e-01,\n",
       "         5.18174867e-01,  -5.41867250e-01,   4.80748158e-01,\n",
       "        -3.31448406e-01,  -1.10030274e+00,   2.13535507e-01,\n",
       "         6.26546479e-01,  -7.40285963e-03,  -1.37911646e-01,\n",
       "        -5.61376567e-01,  -4.69105790e-01,   1.01218186e-01,\n",
       "         1.52489360e+00,   7.29624472e-01,  -2.93767465e-01,\n",
       "        -1.18753161e+00,   3.14005415e-02,  -1.10630388e+00,\n",
       "        -1.04780758e+00,  -6.72084966e-01,  -1.26118275e+00,\n",
       "        -1.60699739e+00,  -3.27833636e-01,   1.64757644e-01,\n",
       "        -7.34057476e-01])], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizedTrainW2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
    "from keras.utils import plot_model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "import os, re, csv, math, codecs\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(0)\n",
    "\n",
    "DATA_PATH = '../input/'\n",
    "EMBEDDING_DIR = '../input/'\n",
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "\n",
    "from subprocess import check_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training CNN ...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, None, 10)          1595710   \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, None, 64)          4544      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, None, 64)          28736     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_6 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 7)                 231       \n",
      "=================================================================\n",
      "Total params: 1,631,301\n",
      "Trainable params: 1,631,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#CNN architecture\n",
    "numClases = 7\n",
    "\n",
    "#training params\n",
    "batch_size = 256 \n",
    "num_epochs = 8 \n",
    "\n",
    "#model parameters\n",
    "num_filters = 64 \n",
    "embed_dim = 300 \n",
    "weight_decay = 1e-4\n",
    "\n",
    "\n",
    "print(\"training CNN ...\")\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(X_train), output_dim=10))\n",
    "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Dense(numClases, activation='sigmoid'))  #multi-label (k-hot encoding)\n",
    "\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/8\n",
      " - 108s - loss: 0.1635 - acc: 0.9541 - val_loss: 0.1641 - val_acc: 0.9534\n",
      "Epoch 2/8\n",
      " - 122s - loss: 0.1612 - acc: 0.9541 - val_loss: 0.1612 - val_acc: 0.9534\n",
      "Epoch 3/8\n",
      " - 114s - loss: 0.1596 - acc: 0.9541 - val_loss: 0.1598 - val_acc: 0.9534\n",
      "Epoch 4/8\n",
      " - 118s - loss: 0.1587 - acc: 0.9541 - val_loss: 0.1587 - val_acc: 0.9534\n",
      "Epoch 5/8\n",
      " - 111s - loss: 0.1585 - acc: 0.9541 - val_loss: 0.1585 - val_acc: 0.9534\n",
      "Epoch 6/8\n",
      " - 112s - loss: 0.1581 - acc: 0.9541 - val_loss: 0.1578 - val_acc: 0.9534\n",
      "Epoch 7/8\n",
      " - 112s - loss: 0.1578 - acc: 0.9541 - val_loss: 0.1574 - val_acc: 0.9534\n",
      "Epoch 8/8\n",
      " - 111s - loss: 0.1574 - acc: 0.9541 - val_loss: 0.1578 - val_acc: 0.9534\n"
     ]
    }
   ],
   "source": [
    "#define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=0, verbose=1)\n",
    "callbacks_list = [early_stopping]\n",
    "#model training\n",
    "hist = model.fit(X_train_bow, y_train, batch_size=batch_size, epochs=num_epochs, validation_split=0.1, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = y_test_pred\n",
    "columns = [\"id\",\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "dfTestPredicted = pd.DataFrame(columns=columns)\n",
    "for x in range(len(test)):\n",
    "    dfTestPredicted.loc[x] = [test['id'][x], predicted[x][1], predicted[x][2], predicted[x][3], predicted[x][4], predicted[x][5], predicted[x][6]]\n",
    "dfTestPredicted.to_csv('../reports/testPred/predTestCNN'+ str(topNFeatures) +'.csv',encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
