{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\envs\\py36\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import ast\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "# Linear Models \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, LSTM, Input, RNN\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.utils import plot_model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>listOfCleanWords</th>\n",
       "      <th>cleanWordsAsText</th>\n",
       "      <th>BagOfWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['explanation', 'edits', 'made', 'username', '...</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['aww', 'match', 'background', 'colour', 'seem...</td>\n",
       "      <td>aww match background colour seemingly stuck th...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['hey', 'man', 'really', 'trying', 'edit', 'wa...</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['make', 'real', 'suggestion', 'improvement', ...</td>\n",
       "      <td>make real suggestion improvement wondered sect...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['sir', 'hero', 'chance', 'remember', 'page']</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>00025465d4725e87</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['congratulation', 'well', 'use', 'tool', 'wel...</td>\n",
       "      <td>congratulation well use tool well talk</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['cocksucker', 'piss', 'around', 'work']</td>\n",
       "      <td>cocksucker piss around work</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>00031b1e95af7921</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['vandalism', 'matt', 'shirvington', 'article'...</td>\n",
       "      <td>vandalism matt shirvington article reverted pl...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>00037261f536c51d</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['sorry', 'word', 'nonsense', 'offensive', 'an...</td>\n",
       "      <td>sorry word nonsense offensive anyway intending...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>00040093b2687caa</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['alignment', 'subject', 'contrary', 'dulithgow']</td>\n",
       "      <td>alignment subject contrary dulithgow</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                id  \\\n",
       "0           0  0000997932d777bf   \n",
       "1           1  000103f0d9cfb60f   \n",
       "2           2  000113f07ec002fd   \n",
       "3           3  0001b41b1c6bb37e   \n",
       "4           4  0001d958c54c6e35   \n",
       "5           5  00025465d4725e87   \n",
       "6           6  0002bcb3da6cb337   \n",
       "7           7  00031b1e95af7921   \n",
       "8           8  00037261f536c51d   \n",
       "9           9  00040093b2687caa   \n",
       "\n",
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0             0   \n",
       "1  D'aww! He matches this background colour I'm s...      0             0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0             0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0             0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0             0   \n",
       "5  \"\\n\\nCongratulations from me as well, use the ...      0             0   \n",
       "6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1             1   \n",
       "7  Your vandalism to the Matt Shirvington article...      0             0   \n",
       "8  Sorry if the word 'nonsense' was offensive to ...      0             0   \n",
       "9  alignment on this subject and which are contra...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  \\\n",
       "0        0       0       0              0   \n",
       "1        0       0       0              0   \n",
       "2        0       0       0              0   \n",
       "3        0       0       0              0   \n",
       "4        0       0       0              0   \n",
       "5        0       0       0              0   \n",
       "6        1       0       1              0   \n",
       "7        0       0       0              0   \n",
       "8        0       0       0              0   \n",
       "9        0       0       0              0   \n",
       "\n",
       "                                    listOfCleanWords  \\\n",
       "0  ['explanation', 'edits', 'made', 'username', '...   \n",
       "1  ['aww', 'match', 'background', 'colour', 'seem...   \n",
       "2  ['hey', 'man', 'really', 'trying', 'edit', 'wa...   \n",
       "3  ['make', 'real', 'suggestion', 'improvement', ...   \n",
       "4      ['sir', 'hero', 'chance', 'remember', 'page']   \n",
       "5  ['congratulation', 'well', 'use', 'tool', 'wel...   \n",
       "6           ['cocksucker', 'piss', 'around', 'work']   \n",
       "7  ['vandalism', 'matt', 'shirvington', 'article'...   \n",
       "8  ['sorry', 'word', 'nonsense', 'offensive', 'an...   \n",
       "9  ['alignment', 'subject', 'contrary', 'dulithgow']   \n",
       "\n",
       "                                    cleanWordsAsText      BagOfWords  \n",
       "0  explanation edits made username hardcore metal...  <class 'dict'>  \n",
       "1  aww match background colour seemingly stuck th...  <class 'dict'>  \n",
       "2  hey man really trying edit war guy constantly ...  <class 'dict'>  \n",
       "3  make real suggestion improvement wondered sect...  <class 'dict'>  \n",
       "4                      sir hero chance remember page  <class 'dict'>  \n",
       "5             congratulation well use tool well talk  <class 'dict'>  \n",
       "6                        cocksucker piss around work  <class 'dict'>  \n",
       "7  vandalism matt shirvington article reverted pl...  <class 'dict'>  \n",
       "8  sorry word nonsense offensive anyway intending...  <class 'dict'>  \n",
       "9               alignment subject contrary dulithgow  <class 'dict'>  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nameTrainCSV = 'trainWithListOfCleanWords'\n",
    "nameTestCSV = 'testWithListOfCleanWords'\n",
    "\n",
    "train = pd.read_csv('../data/processed/' + nameTrainCSV + '.csv', encoding='utf-8')\n",
    "train['BagOfWords'] = dict\n",
    "train.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.836838722229004\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for x in range(len(train)):\n",
    "    train.set_value(col='listOfCleanWords',\n",
    "                index=x,\n",
    "                value=ast.literal_eval(train[\"listOfCleanWords\"][x]))\n",
    "    train.set_value(col='BagOfWords',\n",
    "                index=x,\n",
    "                value=Counter(train[\"listOfCleanWords\"][x]))\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>listOfCleanWords</th>\n",
       "      <th>cleanWordsAsText</th>\n",
       "      <th>BagOfWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[explanation, edits, made, username, hardcore,...</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>{'explanation': 1, 'edits': 1, 'made': 1, 'use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[aww, match, background, colour, seemingly, st...</td>\n",
       "      <td>aww match background colour seemingly stuck th...</td>\n",
       "      <td>{'aww': 1, 'match': 1, 'background': 1, 'colou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[hey, man, really, trying, edit, war, guy, con...</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>{'hey': 1, 'man': 1, 'really': 1, 'trying': 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[make, real, suggestion, improvement, wondered...</td>\n",
       "      <td>make real suggestion improvement wondered sect...</td>\n",
       "      <td>{'make': 1, 'real': 1, 'suggestion': 1, 'impro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[sir, hero, chance, remember, page]</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>{'sir': 1, 'hero': 1, 'chance': 1, 'remember':...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                id  \\\n",
       "0           0  0000997932d777bf   \n",
       "1           1  000103f0d9cfb60f   \n",
       "2           2  000113f07ec002fd   \n",
       "3           3  0001b41b1c6bb37e   \n",
       "4           4  0001d958c54c6e35   \n",
       "\n",
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0             0   \n",
       "1  D'aww! He matches this background colour I'm s...      0             0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0             0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0             0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  \\\n",
       "0        0       0       0              0   \n",
       "1        0       0       0              0   \n",
       "2        0       0       0              0   \n",
       "3        0       0       0              0   \n",
       "4        0       0       0              0   \n",
       "\n",
       "                                    listOfCleanWords  \\\n",
       "0  [explanation, edits, made, username, hardcore,...   \n",
       "1  [aww, match, background, colour, seemingly, st...   \n",
       "2  [hey, man, really, trying, edit, war, guy, con...   \n",
       "3  [make, real, suggestion, improvement, wondered...   \n",
       "4                [sir, hero, chance, remember, page]   \n",
       "\n",
       "                                    cleanWordsAsText  \\\n",
       "0  explanation edits made username hardcore metal...   \n",
       "1  aww match background colour seemingly stuck th...   \n",
       "2  hey man really trying edit war guy constantly ...   \n",
       "3  make real suggestion improvement wondered sect...   \n",
       "4                      sir hero chance remember page   \n",
       "\n",
       "                                          BagOfWords  \n",
       "0  {'explanation': 1, 'edits': 1, 'made': 1, 'use...  \n",
       "1  {'aww': 1, 'match': 1, 'background': 1, 'colou...  \n",
       "2  {'hey': 1, 'man': 1, 'really': 1, 'trying': 1,...  \n",
       "3  {'make': 1, 'real': 1, 'suggestion': 1, 'impro...  \n",
       "4  {'sir': 1, 'hero': 1, 'chance': 1, 'remember':...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MULTICLASS PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classLabel = {\n",
    "    0: \"neutral\",\n",
    "    1: \"toxic\",\n",
    "    2 : \"severe_toxic\",\n",
    "    3 : \"obscene\",\n",
    "    4 : \"threat\",\n",
    "    5 : \"insult\",\n",
    "    6 : \"identity_hate\" \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.empty((len(train['cleanWordsAsText']),),dtype=object)\n",
    "allTextToxicTrain = dict()\n",
    "for idx in classLabel:\n",
    "    if classLabel[idx] != \"neutral\":\n",
    "        T = np.where(train[classLabel[idx]] == 1)[0]\n",
    "        allTextToxicTrain[idx] = T\n",
    "        for i in T:\n",
    "            if y[i] is None:\n",
    "                y[i] = [idx]                \n",
    "            else:\n",
    "                y[i].append(idx)\n",
    "indxsOfNeutralTexts = np.where(y == None) \n",
    "y[indxsOfNeutralTexts] = [[0]]\n",
    "indxsOfNeutralTexts = indxsOfNeutralTexts[0]\n",
    "\n",
    "allTextsNoToxicTrain = [str(train['cleanWordsAsText'][x]) for x in indxsOfNeutralTexts]\n",
    "\n",
    "idxList = []\n",
    "for i in allTextToxicTrain.keys():\n",
    "    #allTextToxicTrain[i] = [str(train['cleanWordsAsText'][j]) for j in allTextToxicTrain[i]]\n",
    "    idxList = np.unique(np.append(idxList, allTextToxicTrain[i]))\n",
    "allTextToxicTrain = [str(train['cleanWordsAsText'][j]) for j in idxList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>listOfCleanWords</th>\n",
       "      <th>cleanWordsAsText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "      <td>['yo', 'bitch', 'ja', 'rule', 'succesful', 'ev...</td>\n",
       "      <td>yo bitch ja rule succesful ever whats hating s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "      <td>['rfc', 'title', 'fine', 'imo']</td>\n",
       "      <td>rfc title fine imo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "      <td>['source', 'zawe', 'ashton', 'lapland']</td>\n",
       "      <td>source zawe ashton lapland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "      <td>['look', 'back', 'source', 'information', 'upd...</td>\n",
       "      <td>look back source information updated correct f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "      <td>['anonymously', 'edit', 'article']</td>\n",
       "      <td>anonymously edit article</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                id  \\\n",
       "0           0  00001cee341fdb12   \n",
       "1           1  0000247867823ef7   \n",
       "2           2  00013b17ad220c46   \n",
       "3           3  00017563c3f7919a   \n",
       "4           4  00017695ad8997eb   \n",
       "\n",
       "                                        comment_text  \\\n",
       "0  Yo bitch Ja Rule is more succesful then you'll...   \n",
       "1  == From RfC == \\n\\n The title is fine as it is...   \n",
       "2  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...   \n",
       "3  :If you have a look back at the source, the in...   \n",
       "4          I don't anonymously edit articles at all.   \n",
       "\n",
       "                                    listOfCleanWords  \\\n",
       "0  ['yo', 'bitch', 'ja', 'rule', 'succesful', 'ev...   \n",
       "1                    ['rfc', 'title', 'fine', 'imo']   \n",
       "2            ['source', 'zawe', 'ashton', 'lapland']   \n",
       "3  ['look', 'back', 'source', 'information', 'upd...   \n",
       "4                 ['anonymously', 'edit', 'article']   \n",
       "\n",
       "                                    cleanWordsAsText  \n",
       "0  yo bitch ja rule succesful ever whats hating s...  \n",
       "1                                 rfc title fine imo  \n",
       "2                         source zawe ashton lapland  \n",
       "3  look back source information updated correct f...  \n",
       "4                           anonymously edit article  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test clasification\n",
    "test = pd.read_csv('../data/processed/' + nameTestCSV + '.csv', encoding='utf-8')\n",
    "test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allTrainText = [txt if txt is not np.nan else '' for txt in train['cleanWordsAsText']]\n",
    "allTestText = [txt if txt is not np.nan else '' for txt in test['cleanWordsAsText']]\n",
    "X_train = allTrainText\n",
    "X_test = allTestText\n",
    "yBinary = MultiLabelBinarizer().fit_transform(y)\n",
    "y_train = yBinary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = [\"idExp\",\"numFeatures\", \"algorithm\", \"Nfolds\", \"accuaracy\", \"logloss\", \"fmeasure\"]\n",
    "dfTestResults = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxFeatures = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NFeatures = 100000\n"
     ]
    }
   ],
   "source": [
    "# Vectorizer all text\n",
    "tfidV = TfidfVectorizer(ngram_range=(1,6), max_features=maxFeatures)\n",
    "X_train_tfid = tfidV.fit_transform(allTrainText)\n",
    "\n",
    "# Fit all clasificators with tfid matrix\n",
    "numFeatures = len(tfidV.get_feature_names())\n",
    "print(\"NFeatures = \" + str(numFeatures))\n",
    "tfidVTest = TfidfVectorizer(vocabulary=tfidV.get_feature_names())\n",
    "X_test_tfid = tfidVTest.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOW\n",
    "bowFeatures = CountVectorizer(vocabulary=tfidV.get_feature_names())\n",
    "X_train_bow = bowFeatures.fit_transform(allTrainText)\n",
    "\n",
    "X_test_bow = bowFeatures.fit_transform(allTestText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "\n",
    "tokenizer = Tokenizer(num_words=maxFeatures)\n",
    "tokenizer.fit_on_texts(list(allTrainText))\n",
    "X_train_tokenized_seq = tokenizer.texts_to_sequences(allTrainText)\n",
    "X_test_tokenized_seq = tokenizer.texts_to_sequences(allTestText)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158652"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq = pad_sequences(X_train_tokenized_seq)\n",
    "X_test_seq = pad_sequences(X_test_tokenized_seq, maxlen=len(X_train_seq[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1250"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0, ...,    2,   69, 2933])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         15957100  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 128)         89728     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 128)         114816    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 231       \n",
      "=================================================================\n",
      "Total params: 16,166,003\n",
      "Trainable params: 16,166,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# MODEL CNN\n",
    "numClases = 7\n",
    "\n",
    "#training params\n",
    "batch_size = 512 \n",
    "num_epochs = 8 \n",
    "\n",
    "#model parameters\n",
    "num_filters = 128 \n",
    "weight_decay = 1e-4\n",
    "outputDim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(X_train), output_dim=outputDim))\n",
    "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Dense(numClases, activation='sigmoid'))  #multi-label (k-hot encoding)\n",
    "\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-b8cc9bb85b56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#model training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcnnmModelHist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_bow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 960\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1657\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2357\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1087\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1088\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1089\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32mC:\\Anaconda2\\envs\\py36\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \"\"\"\n\u001b[1;32m--> 531\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "#model training\n",
    "cnnmModelHist = model.fit(X_train_seq, y_train, batch_size=batch_size, epochs=num_epochs, validation_split=0.1, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 6067/153164 [00:09<04:00, 610.71it/s]C:\\Anaconda2\\envs\\py36\\lib\\site-packages\\tqdm\\_monitor.py:89: TqdmSynchronisationWarning: Set changed size during iteration (see https://github.com/tqdm/tqdm/issues/481)\n",
      "  TqdmSynchronisationWarning)\n",
      "100%|██████████| 153164/153164 [24:40<00:00, 103.45it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'topNFeatures' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-02ebe1d20dfd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdfTestPredicted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdfTestPredicted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../reports/testPred/predTestCNN_Seq_'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopNFeatures\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'topNFeatures' is not defined"
     ]
    }
   ],
   "source": [
    "predicted = y_test_pred\n",
    "columns = [\"id\",\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "dfTestPredicted = pd.DataFrame(columns=columns)\n",
    "for x in tqdm(range(len(test))):\n",
    "    dfTestPredicted.loc[x] = [test['id'][x], predicted[x][1], predicted[x][2], predicted[x][3], predicted[x][4], predicted[x][5], predicted[x][6]]\n",
    "dfTestPredicted.to_csv('../reports/testPred/predTestCNN_Seq_'+ str(maxFeatures) +'.csv',encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfTestPredicted.to_csv('../reports/testPred/predTestCNN_Seq_'+ str(maxFeatures) +'.csv',encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(len(X_train_seq[0]), ))\n",
    "embed_size = 128\n",
    "x = Embedding(maxFeatures, embed_size)(inp)\n",
    "x = LSTM(90, return_sequences=True,name='lstm_layer')(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(60, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(7, activation=\"sigmoid\")(x)\n",
    "modelLSTM = Model(inputs=inp, outputs=x)\n",
    "modelLSTM.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 98816/143613 [===================>..........] - ETA: 1:53:32 - loss: 0.6940 - acc: 0.42 - ETA: 1:32:23 - loss: 0.6911 - acc: 0.59 - ETA: 1:26:24 - loss: 0.6884 - acc: 0.69 - ETA: 1:23:13 - loss: 0.6857 - acc: 0.74 - ETA: 1:21:52 - loss: 0.6827 - acc: 0.78 - ETA: 1:21:04 - loss: 0.6796 - acc: 0.81 - ETA: 1:20:50 - loss: 0.6759 - acc: 0.83 - ETA: 1:21:10 - loss: 0.6716 - acc: 0.84 - ETA: 1:21:52 - loss: 0.6668 - acc: 0.85 - ETA: 1:22:14 - loss: 0.6604 - acc: 0.86 - ETA: 1:23:03 - loss: 0.6524 - acc: 0.86 - ETA: 1:23:46 - loss: 0.6424 - acc: 0.87 - ETA: 1:24:31 - loss: 0.6298 - acc: 0.87 - ETA: 1:25:10 - loss: 0.6165 - acc: 0.87 - ETA: 1:25:58 - loss: 0.6020 - acc: 0.88 - ETA: 1:26:36 - loss: 0.5868 - acc: 0.88 - ETA: 1:27:36 - loss: 0.5717 - acc: 0.88 - ETA: 1:28:12 - loss: 0.5565 - acc: 0.89 - ETA: 1:28:45 - loss: 0.5425 - acc: 0.89 - ETA: 1:29:14 - loss: 0.5295 - acc: 0.89 - ETA: 1:29:49 - loss: 0.5177 - acc: 0.90 - ETA: 1:31:17 - loss: 0.5049 - acc: 0.90 - ETA: 1:32:06 - loss: 0.4931 - acc: 0.90 - ETA: 1:32:49 - loss: 0.4822 - acc: 0.90 - ETA: 1:33:34 - loss: 0.4721 - acc: 0.90 - ETA: 1:34:30 - loss: 0.4621 - acc: 0.91 - ETA: 1:35:13 - loss: 0.4527 - acc: 0.91 - ETA: 1:36:03 - loss: 0.4442 - acc: 0.91 - ETA: 1:36:53 - loss: 0.4351 - acc: 0.91 - ETA: 1:37:40 - loss: 0.4263 - acc: 0.91 - ETA: 1:38:27 - loss: 0.4191 - acc: 0.91 - ETA: 1:39:16 - loss: 0.4119 - acc: 0.91 - ETA: 1:40:17 - loss: 0.4064 - acc: 0.91 - ETA: 1:41:01 - loss: 0.3995 - acc: 0.92 - ETA: 1:41:49 - loss: 0.3949 - acc: 0.92 - ETA: 1:42:38 - loss: 0.3898 - acc: 0.92 - ETA: 1:43:17 - loss: 0.3840 - acc: 0.92 - ETA: 1:44:00 - loss: 0.3799 - acc: 0.92 - ETA: 1:44:41 - loss: 0.3750 - acc: 0.92 - ETA: 1:45:18 - loss: 0.3702 - acc: 0.92 - ETA: 1:46:01 - loss: 0.3660 - acc: 0.92 - ETA: 1:46:46 - loss: 0.3617 - acc: 0.92 - ETA: 1:47:17 - loss: 0.3576 - acc: 0.92 - ETA: 1:47:38 - loss: 0.3525 - acc: 0.92 - ETA: 1:48:07 - loss: 0.3480 - acc: 0.92 - ETA: 1:48:28 - loss: 0.3438 - acc: 0.92 - ETA: 1:48:44 - loss: 0.3400 - acc: 0.93 - ETA: 1:48:58 - loss: 0.3360 - acc: 0.93 - ETA: 1:49:13 - loss: 0.3326 - acc: 0.93 - ETA: 1:49:31 - loss: 0.3291 - acc: 0.93 - ETA: 1:49:49 - loss: 0.3267 - acc: 0.93 - ETA: 1:50:02 - loss: 0.3233 - acc: 0.93 - ETA: 1:50:20 - loss: 0.3206 - acc: 0.93 - ETA: 1:50:35 - loss: 0.3177 - acc: 0.93 - ETA: 1:50:50 - loss: 0.3152 - acc: 0.93 - ETA: 1:51:03 - loss: 0.3131 - acc: 0.93 - ETA: 1:51:17 - loss: 0.3106 - acc: 0.93 - ETA: 1:51:27 - loss: 0.3087 - acc: 0.93 - ETA: 1:51:47 - loss: 0.3067 - acc: 0.93 - ETA: 1:52:11 - loss: 0.3045 - acc: 0.93 - ETA: 1:52:46 - loss: 0.3022 - acc: 0.93 - ETA: 1:53:20 - loss: 0.3006 - acc: 0.93 - ETA: 1:53:38 - loss: 0.2984 - acc: 0.93 - ETA: 1:54:00 - loss: 0.2964 - acc: 0.93 - ETA: 1:54:14 - loss: 0.2948 - acc: 0.93 - ETA: 1:54:29 - loss: 0.2931 - acc: 0.93 - ETA: 1:54:49 - loss: 0.2915 - acc: 0.93 - ETA: 1:55:15 - loss: 0.2900 - acc: 0.93 - ETA: 1:55:58 - loss: 0.2884 - acc: 0.93 - ETA: 1:56:44 - loss: 0.2870 - acc: 0.93 - ETA: 1:57:16 - loss: 0.2853 - acc: 0.93 - ETA: 1:57:47 - loss: 0.2834 - acc: 0.93 - ETA: 1:58:09 - loss: 0.2822 - acc: 0.93 - ETA: 1:58:47 - loss: 0.2806 - acc: 0.93 - ETA: 1:59:59 - loss: 0.2792 - acc: 0.93 - ETA: 2:02:23 - loss: 0.2777 - acc: 0.93 - ETA: 2:03:39 - loss: 0.2763 - acc: 0.93 - ETA: 2:04:29 - loss: 0.2752 - acc: 0.93 - ETA: 2:04:50 - loss: 0.2738 - acc: 0.93 - ETA: 2:06:22 - loss: 0.2723 - acc: 0.94 - ETA: 2:06:49 - loss: 0.2714 - acc: 0.94 - ETA: 2:07:06 - loss: 0.2699 - acc: 0.94 - ETA: 2:07:20 - loss: 0.2688 - acc: 0.94 - ETA: 2:07:37 - loss: 0.2677 - acc: 0.94 - ETA: 2:07:55 - loss: 0.2674 - acc: 0.94 - ETA: 2:08:00 - loss: 0.2661 - acc: 0.94 - ETA: 2:08:12 - loss: 0.2649 - acc: 0.94 - ETA: 2:08:19 - loss: 0.2642 - acc: 0.94 - ETA: 2:08:21 - loss: 0.2631 - acc: 0.94 - ETA: 2:08:28 - loss: 0.2621 - acc: 0.94 - ETA: 2:08:27 - loss: 0.2611 - acc: 0.94 - ETA: 2:08:25 - loss: 0.2599 - acc: 0.94 - ETA: 2:08:24 - loss: 0.2587 - acc: 0.94 - ETA: 2:08:21 - loss: 0.2579 - acc: 0.94 - ETA: 2:08:18 - loss: 0.2569 - acc: 0.94 - ETA: 2:08:16 - loss: 0.2557 - acc: 0.94 - ETA: 2:08:13 - loss: 0.2548 - acc: 0.94 - ETA: 2:08:11 - loss: 0.2538 - acc: 0.94 - ETA: 2:08:13 - loss: 0.2526 - acc: 0.94 - ETA: 2:09:17 - loss: 0.2519 - acc: 0.94 - ETA: 2:09:19 - loss: 0.2509 - acc: 0.94 - ETA: 2:09:54 - loss: 0.2501 - acc: 0.94 - ETA: 2:10:00 - loss: 0.2492 - acc: 0.94 - ETA: 2:10:51 - loss: 0.2482 - acc: 0.94 - ETA: 2:11:18 - loss: 0.2472 - acc: 0.94 - ETA: 2:11:08 - loss: 0.2467 - acc: 0.94 - ETA: 2:10:58 - loss: 0.2460 - acc: 0.94 - ETA: 2:10:46 - loss: 0.2454 - acc: 0.94 - ETA: 2:10:47 - loss: 0.2451 - acc: 0.94 - ETA: 2:11:37 - loss: 0.2444 - acc: 0.94 - ETA: 2:11:51 - loss: 0.2437 - acc: 0.94 - ETA: 2:11:36 - loss: 0.2431 - acc: 0.94 - ETA: 2:11:22 - loss: 0.2424 - acc: 0.94 - ETA: 2:11:07 - loss: 0.2420 - acc: 0.94 - ETA: 2:10:49 - loss: 0.2414 - acc: 0.94 - ETA: 2:10:29 - loss: 0.2408 - acc: 0.94 - ETA: 2:10:10 - loss: 0.2404 - acc: 0.94 - ETA: 2:09:49 - loss: 0.2398 - acc: 0.94 - ETA: 2:09:28 - loss: 0.2392 - acc: 0.94 - ETA: 2:09:07 - loss: 0.2386 - acc: 0.94 - ETA: 2:08:46 - loss: 0.2380 - acc: 0.94 - ETA: 2:08:24 - loss: 0.2375 - acc: 0.94 - ETA: 2:08:01 - loss: 0.2372 - acc: 0.94 - ETA: 2:07:39 - loss: 0.2368 - acc: 0.94 - ETA: 2:07:17 - loss: 0.2364 - acc: 0.94 - ETA: 2:06:54 - loss: 0.2357 - acc: 0.94 - ETA: 2:06:31 - loss: 0.2351 - acc: 0.94 - ETA: 2:06:07 - loss: 0.2345 - acc: 0.94 - ETA: 2:05:44 - loss: 0.2340 - acc: 0.94 - ETA: 2:05:19 - loss: 0.2337 - acc: 0.94 - ETA: 2:04:55 - loss: 0.2333 - acc: 0.94 - ETA: 2:04:30 - loss: 0.2327 - acc: 0.94 - ETA: 2:04:05 - loss: 0.2321 - acc: 0.94 - ETA: 2:03:40 - loss: 0.2317 - acc: 0.94 - ETA: 2:03:14 - loss: 0.2313 - acc: 0.94 - ETA: 2:02:48 - loss: 0.2308 - acc: 0.94 - ETA: 2:02:21 - loss: 0.2304 - acc: 0.94 - ETA: 2:01:54 - loss: 0.2297 - acc: 0.94 - ETA: 2:01:27 - loss: 0.2293 - acc: 0.94 - ETA: 2:00:59 - loss: 0.2288 - acc: 0.94 - ETA: 2:00:31 - loss: 0.2283 - acc: 0.94 - ETA: 2:00:02 - loss: 0.2278 - acc: 0.94 - ETA: 1:59:34 - loss: 0.2275 - acc: 0.94 - ETA: 1:59:04 - loss: 0.2270 - acc: 0.94 - ETA: 1:58:35 - loss: 0.2269 - acc: 0.94 - ETA: 1:58:05 - loss: 0.2264 - acc: 0.94 - ETA: 1:57:35 - loss: 0.2259 - acc: 0.94 - ETA: 1:57:04 - loss: 0.2256 - acc: 0.94 - ETA: 1:56:33 - loss: 0.2252 - acc: 0.94 - ETA: 1:56:02 - loss: 0.2247 - acc: 0.94 - ETA: 1:55:30 - loss: 0.2241 - acc: 0.94 - ETA: 1:54:58 - loss: 0.2237 - acc: 0.94 - ETA: 1:54:27 - loss: 0.2232 - acc: 0.94 - ETA: 1:53:54 - loss: 0.2232 - acc: 0.94 - ETA: 1:53:21 - loss: 0.2230 - acc: 0.94 - ETA: 1:52:47 - loss: 0.2228 - acc: 0.94 - ETA: 1:52:13 - loss: 0.2225 - acc: 0.94 - ETA: 1:51:39 - loss: 0.2222 - acc: 0.94 - ETA: 1:51:04 - loss: 0.2217 - acc: 0.94 - ETA: 1:50:29 - loss: 0.2214 - acc: 0.94 - ETA: 1:49:53 - loss: 0.2211 - acc: 0.94 - ETA: 1:49:17 - loss: 0.2208 - acc: 0.94 - ETA: 1:48:41 - loss: 0.2208 - acc: 0.94 - ETA: 1:48:04 - loss: 0.2204 - acc: 0.94 - ETA: 1:47:28 - loss: 0.2201 - acc: 0.94 - ETA: 1:46:50 - loss: 0.2199 - acc: 0.94 - ETA: 1:46:12 - loss: 0.2195 - acc: 0.94 - ETA: 1:45:34 - loss: 0.2192 - acc: 0.94 - ETA: 1:44:55 - loss: 0.2188 - acc: 0.94 - ETA: 1:44:17 - loss: 0.2185 - acc: 0.94 - ETA: 1:43:37 - loss: 0.2182 - acc: 0.94 - ETA: 1:42:58 - loss: 0.2181 - acc: 0.94 - ETA: 1:42:18 - loss: 0.2180 - acc: 0.94 - ETA: 1:41:38 - loss: 0.2177 - acc: 0.94 - ETA: 1:40:57 - loss: 0.2175 - acc: 0.94 - ETA: 1:40:16 - loss: 0.2171 - acc: 0.94 - ETA: 1:39:35 - loss: 0.2169 - acc: 0.94 - ETA: 1:38:54 - loss: 0.2167 - acc: 0.94 - ETA: 1:38:12 - loss: 0.2164 - acc: 0.94 - ETA: 1:37:29 - loss: 0.2162 - acc: 0.94 - ETA: 1:36:46 - loss: 0.2159 - acc: 0.94 - ETA: 1:36:03 - loss: 0.2156 - acc: 0.94 - ETA: 1:35:20 - loss: 0.2155 - acc: 0.94 - ETA: 1:34:36 - loss: 0.2154 - acc: 0.94 - ETA: 1:33:51 - loss: 0.2152 - acc: 0.94 - ETA: 1:33:07 - loss: 0.2149 - acc: 0.94 - ETA: 1:32:21 - loss: 0.2147 - acc: 0.94 - ETA: 1:31:36 - loss: 0.2145 - acc: 0.94 - ETA: 1:30:50 - loss: 0.2141 - acc: 0.94 - ETA: 1:30:04 - loss: 0.2140 - acc: 0.94 - ETA: 1:29:18 - loss: 0.2137 - acc: 0.94 - ETA: 1:28:31 - loss: 0.2134 - acc: 0.94 - ETA: 1:27:44 - loss: 0.2133 - acc: 0.9485\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143613/143613 [==============================] - ETA: 1:26:57 - loss: 0.2131 - acc: 0.94 - ETA: 1:26:10 - loss: 0.2129 - acc: 0.94 - ETA: 1:25:22 - loss: 0.2128 - acc: 0.94 - ETA: 1:24:33 - loss: 0.2125 - acc: 0.94 - ETA: 1:23:45 - loss: 0.2123 - acc: 0.94 - ETA: 1:22:56 - loss: 0.2120 - acc: 0.94 - ETA: 1:22:06 - loss: 0.2119 - acc: 0.94 - ETA: 1:21:17 - loss: 0.2116 - acc: 0.94 - ETA: 1:20:27 - loss: 0.2114 - acc: 0.94 - ETA: 1:19:37 - loss: 0.2113 - acc: 0.94 - ETA: 1:18:46 - loss: 0.2110 - acc: 0.94 - ETA: 1:17:56 - loss: 0.2108 - acc: 0.94 - ETA: 1:17:05 - loss: 0.2106 - acc: 0.94 - ETA: 1:16:14 - loss: 0.2104 - acc: 0.94 - ETA: 1:15:22 - loss: 0.2102 - acc: 0.94 - ETA: 1:14:30 - loss: 0.2098 - acc: 0.94 - ETA: 1:13:38 - loss: 0.2096 - acc: 0.94 - ETA: 1:12:45 - loss: 0.2094 - acc: 0.94 - ETA: 1:11:52 - loss: 0.2092 - acc: 0.94 - ETA: 1:10:59 - loss: 0.2091 - acc: 0.94 - ETA: 1:10:05 - loss: 0.2089 - acc: 0.94 - ETA: 1:09:12 - loss: 0.2089 - acc: 0.94 - ETA: 1:08:17 - loss: 0.2086 - acc: 0.94 - ETA: 1:07:23 - loss: 0.2085 - acc: 0.94 - ETA: 1:06:29 - loss: 0.2083 - acc: 0.94 - ETA: 1:05:34 - loss: 0.2081 - acc: 0.94 - ETA: 1:04:38 - loss: 0.2079 - acc: 0.94 - ETA: 1:03:43 - loss: 0.2078 - acc: 0.94 - ETA: 1:02:47 - loss: 0.2075 - acc: 0.94 - ETA: 1:01:51 - loss: 0.2073 - acc: 0.94 - ETA: 1:00:55 - loss: 0.2071 - acc: 0.94 - ETA: 59:58 - loss: 0.2069 - acc: 0.9491 - ETA: 59:00 - loss: 0.2067 - acc: 0.94 - ETA: 58:03 - loss: 0.2065 - acc: 0.94 - ETA: 57:05 - loss: 0.2062 - acc: 0.94 - ETA: 56:07 - loss: 0.2059 - acc: 0.94 - ETA: 55:09 - loss: 0.2057 - acc: 0.94 - ETA: 54:10 - loss: 0.2055 - acc: 0.94 - ETA: 53:11 - loss: 0.2053 - acc: 0.94 - ETA: 52:12 - loss: 0.2051 - acc: 0.94 - ETA: 51:12 - loss: 0.2050 - acc: 0.94 - ETA: 50:13 - loss: 0.2048 - acc: 0.94 - ETA: 49:12 - loss: 0.2046 - acc: 0.94 - ETA: 48:12 - loss: 0.2044 - acc: 0.94 - ETA: 47:11 - loss: 0.2041 - acc: 0.94 - ETA: 46:10 - loss: 0.2039 - acc: 0.94 - ETA: 45:09 - loss: 0.2036 - acc: 0.94 - ETA: 44:07 - loss: 0.2034 - acc: 0.94 - ETA: 43:05 - loss: 0.2032 - acc: 0.94 - ETA: 42:03 - loss: 0.2029 - acc: 0.94 - ETA: 41:01 - loss: 0.2028 - acc: 0.94 - ETA: 39:58 - loss: 0.2026 - acc: 0.94 - ETA: 38:55 - loss: 0.2023 - acc: 0.94 - ETA: 37:52 - loss: 0.2020 - acc: 0.94 - ETA: 36:48 - loss: 0.2018 - acc: 0.94 - ETA: 35:44 - loss: 0.2016 - acc: 0.94 - ETA: 34:40 - loss: 0.2013 - acc: 0.94 - ETA: 33:36 - loss: 0.2011 - acc: 0.94 - ETA: 32:31 - loss: 0.2009 - acc: 0.94 - ETA: 31:26 - loss: 0.2006 - acc: 0.94 - ETA: 30:21 - loss: 0.2004 - acc: 0.94 - ETA: 29:16 - loss: 0.2001 - acc: 0.94 - ETA: 28:10 - loss: 0.1999 - acc: 0.94 - ETA: 27:04 - loss: 0.1998 - acc: 0.94 - ETA: 25:58 - loss: 0.1996 - acc: 0.94 - ETA: 24:51 - loss: 0.1994 - acc: 0.94 - ETA: 23:44 - loss: 0.1992 - acc: 0.94 - ETA: 22:37 - loss: 0.1991 - acc: 0.94 - ETA: 21:30 - loss: 0.1987 - acc: 0.94 - ETA: 20:22 - loss: 0.1985 - acc: 0.94 - ETA: 19:15 - loss: 0.1983 - acc: 0.94 - ETA: 18:07 - loss: 0.1981 - acc: 0.94 - ETA: 16:58 - loss: 0.1979 - acc: 0.95 - ETA: 15:50 - loss: 0.1976 - acc: 0.95 - ETA: 14:41 - loss: 0.1974 - acc: 0.95 - ETA: 13:32 - loss: 0.1973 - acc: 0.95 - ETA: 12:22 - loss: 0.1971 - acc: 0.95 - ETA: 11:13 - loss: 0.1968 - acc: 0.95 - ETA: 10:03 - loss: 0.1966 - acc: 0.95 - ETA: 8:53 - loss: 0.1965 - acc: 0.9500 - ETA: 7:42 - loss: 0.1962 - acc: 0.950 - ETA: 6:32 - loss: 0.1960 - acc: 0.950 - ETA: 5:21 - loss: 0.1959 - acc: 0.950 - ETA: 4:10 - loss: 0.1959 - acc: 0.950 - ETA: 2:58 - loss: 0.1958 - acc: 0.950 - ETA: 1:47 - loss: 0.1955 - acc: 0.950 - ETA: 35s - loss: 0.1954 - acc: 0.950 - 20504s 143ms/step - loss: 0.1954 - acc: 0.9500 - val_loss: 0.1373 - val_acc: 0.9534\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99328/143613 [===================>..........] - ETA: 8:11:53 - loss: 0.1419 - acc: 0.95 - ETA: 8:09:24 - loss: 0.1345 - acc: 0.95 - ETA: 8:07:32 - loss: 0.1419 - acc: 0.95 - ETA: 8:05:59 - loss: 0.1405 - acc: 0.95 - ETA: 8:05:04 - loss: 0.1386 - acc: 0.95 - ETA: 8:03:37 - loss: 0.1422 - acc: 0.95 - ETA: 8:01:45 - loss: 0.1433 - acc: 0.95 - ETA: 7:59:54 - loss: 0.1448 - acc: 0.95 - ETA: 7:58:15 - loss: 0.1433 - acc: 0.95 - ETA: 7:56:33 - loss: 0.1403 - acc: 0.95 - ETA: 7:54:49 - loss: 0.1386 - acc: 0.95 - ETA: 7:53:11 - loss: 0.1387 - acc: 0.95 - ETA: 7:51:31 - loss: 0.1369 - acc: 0.95 - ETA: 7:49:55 - loss: 0.1386 - acc: 0.95 - ETA: 7:48:18 - loss: 0.1375 - acc: 0.95 - ETA: 7:46:39 - loss: 0.1371 - acc: 0.95 - ETA: 7:45:08 - loss: 0.1371 - acc: 0.95 - ETA: 7:43:34 - loss: 0.1373 - acc: 0.95 - ETA: 7:42:08 - loss: 0.1358 - acc: 0.95 - ETA: 7:40:35 - loss: 0.1362 - acc: 0.95 - ETA: 7:39:00 - loss: 0.1358 - acc: 0.95 - ETA: 7:37:22 - loss: 0.1349 - acc: 0.95 - ETA: 7:35:41 - loss: 0.1351 - acc: 0.95 - ETA: 7:34:03 - loss: 0.1343 - acc: 0.95 - ETA: 7:32:29 - loss: 0.1340 - acc: 0.95 - ETA: 7:30:53 - loss: 0.1343 - acc: 0.95 - ETA: 7:29:19 - loss: 0.1352 - acc: 0.95 - ETA: 7:27:45 - loss: 0.1349 - acc: 0.95 - ETA: 7:26:09 - loss: 0.1343 - acc: 0.95 - ETA: 7:24:35 - loss: 0.1345 - acc: 0.95 - ETA: 7:23:04 - loss: 0.1340 - acc: 0.95 - ETA: 7:21:32 - loss: 0.1338 - acc: 0.95 - ETA: 7:19:53 - loss: 0.1333 - acc: 0.95 - ETA: 7:18:16 - loss: 0.1327 - acc: 0.95 - ETA: 7:16:38 - loss: 0.1325 - acc: 0.95 - ETA: 7:15:01 - loss: 0.1325 - acc: 0.95 - ETA: 7:13:22 - loss: 0.1328 - acc: 0.95 - ETA: 7:11:42 - loss: 0.1332 - acc: 0.95 - ETA: 7:10:08 - loss: 0.1327 - acc: 0.95 - ETA: 7:08:33 - loss: 0.1325 - acc: 0.95 - ETA: 7:06:54 - loss: 0.1323 - acc: 0.95 - ETA: 7:05:15 - loss: 0.1319 - acc: 0.95 - ETA: 7:03:35 - loss: 0.1316 - acc: 0.95 - ETA: 7:01:56 - loss: 0.1313 - acc: 0.95 - ETA: 7:00:16 - loss: 0.1313 - acc: 0.95 - ETA: 6:58:40 - loss: 0.1310 - acc: 0.95 - ETA: 6:57:01 - loss: 0.1308 - acc: 0.95 - ETA: 6:55:20 - loss: 0.1305 - acc: 0.95 - ETA: 6:53:42 - loss: 0.1306 - acc: 0.95 - ETA: 6:52:00 - loss: 0.1306 - acc: 0.95 - ETA: 6:50:22 - loss: 0.1311 - acc: 0.95 - ETA: 6:48:42 - loss: 0.1310 - acc: 0.95 - ETA: 6:47:02 - loss: 0.1310 - acc: 0.95 - ETA: 6:45:25 - loss: 0.1307 - acc: 0.95 - ETA: 6:43:44 - loss: 0.1304 - acc: 0.95 - ETA: 6:42:04 - loss: 0.1308 - acc: 0.95 - ETA: 6:40:24 - loss: 0.1308 - acc: 0.95 - ETA: 6:38:43 - loss: 0.1311 - acc: 0.95 - ETA: 6:37:03 - loss: 0.1310 - acc: 0.95 - ETA: 6:35:21 - loss: 0.1307 - acc: 0.95 - ETA: 6:33:41 - loss: 0.1306 - acc: 0.95 - ETA: 6:32:02 - loss: 0.1303 - acc: 0.95 - ETA: 6:30:20 - loss: 0.1303 - acc: 0.95 - ETA: 6:28:38 - loss: 0.1303 - acc: 0.95 - ETA: 6:26:58 - loss: 0.1301 - acc: 0.95 - ETA: 6:25:15 - loss: 0.1299 - acc: 0.95 - ETA: 6:23:33 - loss: 0.1296 - acc: 0.95 - ETA: 6:21:50 - loss: 0.1294 - acc: 0.95 - ETA: 6:20:06 - loss: 0.1292 - acc: 0.95 - ETA: 6:18:23 - loss: 0.1292 - acc: 0.95 - ETA: 6:16:40 - loss: 0.1289 - acc: 0.95 - ETA: 6:14:55 - loss: 0.1290 - acc: 0.95 - ETA: 6:13:12 - loss: 0.1290 - acc: 0.95 - ETA: 6:11:28 - loss: 0.1286 - acc: 0.95 - ETA: 6:09:45 - loss: 0.1286 - acc: 0.95 - ETA: 6:08:04 - loss: 0.1286 - acc: 0.95 - ETA: 6:06:22 - loss: 0.1281 - acc: 0.95 - ETA: 6:04:37 - loss: 0.1277 - acc: 0.95 - ETA: 6:02:53 - loss: 0.1274 - acc: 0.95 - ETA: 6:01:09 - loss: 0.1272 - acc: 0.95 - ETA: 5:59:25 - loss: 0.1270 - acc: 0.95 - ETA: 5:57:41 - loss: 0.1271 - acc: 0.95 - ETA: 5:55:57 - loss: 0.1268 - acc: 0.95 - ETA: 5:54:13 - loss: 0.1267 - acc: 0.95 - ETA: 5:52:28 - loss: 0.1265 - acc: 0.95 - ETA: 5:50:44 - loss: 0.1261 - acc: 0.95 - ETA: 5:49:00 - loss: 0.1262 - acc: 0.95 - ETA: 5:47:15 - loss: 0.1259 - acc: 0.95 - ETA: 5:45:30 - loss: 0.1257 - acc: 0.95 - ETA: 5:43:46 - loss: 0.1252 - acc: 0.95 - ETA: 5:42:01 - loss: 0.1251 - acc: 0.95 - ETA: 5:40:18 - loss: 0.1248 - acc: 0.95 - ETA: 5:38:36 - loss: 0.1247 - acc: 0.95 - ETA: 5:36:51 - loss: 0.1244 - acc: 0.95 - ETA: 5:35:07 - loss: 0.1241 - acc: 0.95 - ETA: 5:33:21 - loss: 0.1239 - acc: 0.95 - ETA: 5:31:36 - loss: 0.1239 - acc: 0.95 - ETA: 5:29:51 - loss: 0.1236 - acc: 0.95 - ETA: 5:28:05 - loss: 0.1234 - acc: 0.95 - ETA: 5:26:21 - loss: 0.1231 - acc: 0.95 - ETA: 5:24:37 - loss: 0.1231 - acc: 0.95 - ETA: 5:22:52 - loss: 0.1227 - acc: 0.95 - ETA: 5:21:07 - loss: 0.1224 - acc: 0.95 - ETA: 5:19:22 - loss: 0.1225 - acc: 0.95 - ETA: 5:17:35 - loss: 0.1224 - acc: 0.95 - ETA: 5:15:50 - loss: 0.1224 - acc: 0.95 - ETA: 5:14:04 - loss: 0.1222 - acc: 0.95 - ETA: 5:12:18 - loss: 0.1220 - acc: 0.95 - ETA: 5:10:33 - loss: 0.1218 - acc: 0.95 - ETA: 5:08:47 - loss: 0.1216 - acc: 0.95 - ETA: 5:07:01 - loss: 0.1214 - acc: 0.95 - ETA: 5:05:15 - loss: 0.1211 - acc: 0.95 - ETA: 5:03:29 - loss: 0.1210 - acc: 0.95 - ETA: 5:01:44 - loss: 0.1207 - acc: 0.95 - ETA: 4:59:59 - loss: 0.1204 - acc: 0.95 - ETA: 4:58:12 - loss: 0.1202 - acc: 0.95 - ETA: 4:56:27 - loss: 0.1201 - acc: 0.95 - ETA: 4:54:40 - loss: 0.1200 - acc: 0.95 - ETA: 4:52:54 - loss: 0.1197 - acc: 0.95 - ETA: 4:51:07 - loss: 0.1194 - acc: 0.95 - ETA: 4:49:21 - loss: 0.1192 - acc: 0.95 - ETA: 4:47:34 - loss: 0.1190 - acc: 0.95 - ETA: 4:45:54 - loss: 0.1187 - acc: 0.95 - ETA: 4:44:25 - loss: 0.1184 - acc: 0.95 - ETA: 4:42:40 - loss: 0.1182 - acc: 0.95 - ETA: 4:40:54 - loss: 0.1181 - acc: 0.95 - ETA: 4:39:08 - loss: 0.1178 - acc: 0.95 - ETA: 4:37:20 - loss: 0.1176 - acc: 0.95 - ETA: 4:35:32 - loss: 0.1174 - acc: 0.95 - ETA: 4:33:44 - loss: 0.1172 - acc: 0.95 - ETA: 4:31:56 - loss: 0.1170 - acc: 0.95 - ETA: 4:30:09 - loss: 0.1168 - acc: 0.95 - ETA: 4:28:22 - loss: 0.1166 - acc: 0.95 - ETA: 4:26:34 - loss: 0.1162 - acc: 0.95 - ETA: 4:24:47 - loss: 0.1160 - acc: 0.95 - ETA: 4:23:00 - loss: 0.1156 - acc: 0.95 - ETA: 4:21:12 - loss: 0.1154 - acc: 0.95 - ETA: 4:19:24 - loss: 0.1150 - acc: 0.95 - ETA: 4:17:36 - loss: 0.1149 - acc: 0.95 - ETA: 4:15:48 - loss: 0.1146 - acc: 0.95 - ETA: 4:14:01 - loss: 0.1143 - acc: 0.95 - ETA: 4:12:14 - loss: 0.1140 - acc: 0.95 - ETA: 4:10:27 - loss: 0.1138 - acc: 0.95 - ETA: 4:08:39 - loss: 0.1136 - acc: 0.95 - ETA: 4:06:52 - loss: 0.1134 - acc: 0.96 - ETA: 4:05:05 - loss: 0.1132 - acc: 0.96 - ETA: 4:03:17 - loss: 0.1129 - acc: 0.96 - ETA: 4:01:30 - loss: 0.1127 - acc: 0.96 - ETA: 3:59:42 - loss: 0.1125 - acc: 0.96 - ETA: 3:57:54 - loss: 0.1122 - acc: 0.96 - ETA: 3:56:07 - loss: 0.1120 - acc: 0.96 - ETA: 3:54:20 - loss: 0.1117 - acc: 0.96 - ETA: 3:52:32 - loss: 0.1115 - acc: 0.96 - ETA: 3:50:44 - loss: 0.1114 - acc: 0.96 - ETA: 3:48:57 - loss: 0.1112 - acc: 0.96 - ETA: 3:47:09 - loss: 0.1109 - acc: 0.96 - ETA: 3:45:21 - loss: 0.1107 - acc: 0.96 - ETA: 3:43:33 - loss: 0.1105 - acc: 0.96 - ETA: 3:41:45 - loss: 0.1102 - acc: 0.96 - ETA: 3:39:57 - loss: 0.1099 - acc: 0.96 - ETA: 3:38:08 - loss: 0.1097 - acc: 0.96 - ETA: 3:36:20 - loss: 0.1095 - acc: 0.96 - ETA: 3:34:32 - loss: 0.1092 - acc: 0.96 - ETA: 3:32:44 - loss: 0.1090 - acc: 0.96 - ETA: 3:30:56 - loss: 0.1088 - acc: 0.96 - ETA: 3:29:07 - loss: 0.1086 - acc: 0.96 - ETA: 3:27:19 - loss: 0.1084 - acc: 0.96 - ETA: 3:25:31 - loss: 0.1081 - acc: 0.96 - ETA: 3:23:43 - loss: 0.1078 - acc: 0.96 - ETA: 3:21:54 - loss: 0.1076 - acc: 0.96 - ETA: 3:20:06 - loss: 0.1074 - acc: 0.96 - ETA: 3:18:17 - loss: 0.1072 - acc: 0.96 - ETA: 3:16:29 - loss: 0.1069 - acc: 0.96 - ETA: 3:14:40 - loss: 0.1067 - acc: 0.96 - ETA: 3:12:52 - loss: 0.1065 - acc: 0.96 - ETA: 3:11:03 - loss: 0.1063 - acc: 0.96 - ETA: 3:09:15 - loss: 0.1060 - acc: 0.96 - ETA: 3:07:26 - loss: 0.1057 - acc: 0.96 - ETA: 3:05:38 - loss: 0.1055 - acc: 0.96 - ETA: 3:03:52 - loss: 0.1053 - acc: 0.96 - ETA: 3:02:00 - loss: 0.1051 - acc: 0.96 - ETA: 3:00:11 - loss: 0.1049 - acc: 0.96 - ETA: 2:58:21 - loss: 0.1047 - acc: 0.96 - ETA: 2:56:31 - loss: 0.1045 - acc: 0.96 - ETA: 2:54:42 - loss: 0.1043 - acc: 0.96 - ETA: 2:52:51 - loss: 0.1041 - acc: 0.96 - ETA: 2:51:07 - loss: 0.1039 - acc: 0.96 - ETA: 2:49:27 - loss: 0.1037 - acc: 0.96 - ETA: 2:47:48 - loss: 0.1035 - acc: 0.96 - ETA: 2:45:56 - loss: 0.1033 - acc: 0.96 - ETA: 2:44:14 - loss: 0.1032 - acc: 0.96 - ETA: 2:42:32 - loss: 0.1031 - acc: 0.96 - ETA: 2:40:47 - loss: 0.1028 - acc: 0.96 - ETA: 2:39:00 - loss: 0.1026 - acc: 0.9639"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143613/143613 [==============================] - ETA: 2:37:16 - loss: 0.1024 - acc: 0.96 - ETA: 2:35:31 - loss: 0.1023 - acc: 0.96 - ETA: 2:33:47 - loss: 0.1021 - acc: 0.96 - ETA: 2:32:02 - loss: 0.1020 - acc: 0.96 - ETA: 2:30:18 - loss: 0.1018 - acc: 0.96 - ETA: 2:28:36 - loss: 0.1016 - acc: 0.96 - ETA: 2:26:52 - loss: 0.1015 - acc: 0.96 - ETA: 2:25:00 - loss: 0.1014 - acc: 0.96 - ETA: 2:23:08 - loss: 0.1011 - acc: 0.96 - ETA: 2:21:15 - loss: 0.1009 - acc: 0.96 - ETA: 2:19:21 - loss: 0.1007 - acc: 0.96 - ETA: 2:17:28 - loss: 0.1006 - acc: 0.96 - ETA: 2:15:35 - loss: 0.1004 - acc: 0.96 - ETA: 2:13:42 - loss: 0.1003 - acc: 0.96 - ETA: 2:11:50 - loss: 0.1001 - acc: 0.96 - ETA: 2:10:01 - loss: 0.1000 - acc: 0.96 - ETA: 2:08:12 - loss: 0.0998 - acc: 0.96 - ETA: 2:06:23 - loss: 0.0996 - acc: 0.96 - ETA: 2:04:34 - loss: 0.0995 - acc: 0.96 - ETA: 2:02:46 - loss: 0.0993 - acc: 0.96 - ETA: 2:00:58 - loss: 0.0991 - acc: 0.96 - ETA: 1:59:17 - loss: 0.0989 - acc: 0.96 - ETA: 1:57:31 - loss: 0.0987 - acc: 0.96 - ETA: 1:55:43 - loss: 0.0986 - acc: 0.96 - ETA: 1:53:55 - loss: 0.0984 - acc: 0.96 - ETA: 1:52:15 - loss: 0.0984 - acc: 0.96 - ETA: 1:50:29 - loss: 0.0983 - acc: 0.96 - ETA: 1:48:50 - loss: 0.0981 - acc: 0.96 - ETA: 1:47:16 - loss: 0.0980 - acc: 0.96 - ETA: 1:45:40 - loss: 0.0979 - acc: 0.96 - ETA: 1:44:04 - loss: 0.0977 - acc: 0.96 - ETA: 1:42:28 - loss: 0.0976 - acc: 0.96 - ETA: 1:40:51 - loss: 0.0974 - acc: 0.96 - ETA: 1:39:12 - loss: 0.0973 - acc: 0.96 - ETA: 1:37:33 - loss: 0.0972 - acc: 0.96 - ETA: 1:35:54 - loss: 0.0970 - acc: 0.96 - ETA: 1:34:15 - loss: 0.0969 - acc: 0.96 - ETA: 1:32:33 - loss: 0.0967 - acc: 0.96 - ETA: 1:30:52 - loss: 0.0966 - acc: 0.96 - ETA: 1:28:58 - loss: 0.0964 - acc: 0.96 - ETA: 1:27:04 - loss: 0.0963 - acc: 0.96 - ETA: 1:25:10 - loss: 0.0961 - acc: 0.96 - ETA: 1:23:17 - loss: 0.0960 - acc: 0.96 - ETA: 1:21:23 - loss: 0.0958 - acc: 0.96 - ETA: 1:19:30 - loss: 0.0957 - acc: 0.96 - ETA: 1:17:36 - loss: 0.0956 - acc: 0.96 - ETA: 1:15:43 - loss: 0.0954 - acc: 0.96 - ETA: 1:13:49 - loss: 0.0953 - acc: 0.96 - ETA: 1:11:56 - loss: 0.0952 - acc: 0.96 - ETA: 1:10:02 - loss: 0.0950 - acc: 0.96 - ETA: 1:08:09 - loss: 0.0949 - acc: 0.96 - ETA: 1:06:15 - loss: 0.0947 - acc: 0.96 - ETA: 1:04:22 - loss: 0.0946 - acc: 0.96 - ETA: 1:02:28 - loss: 0.0944 - acc: 0.96 - ETA: 1:00:34 - loss: 0.0943 - acc: 0.96 - ETA: 58:39 - loss: 0.0942 - acc: 0.9667 - ETA: 56:46 - loss: 0.0940 - acc: 0.96 - ETA: 54:54 - loss: 0.0940 - acc: 0.96 - ETA: 53:02 - loss: 0.0939 - acc: 0.96 - ETA: 51:08 - loss: 0.0938 - acc: 0.96 - ETA: 49:13 - loss: 0.0937 - acc: 0.96 - ETA: 47:18 - loss: 0.0936 - acc: 0.96 - ETA: 45:23 - loss: 0.0935 - acc: 0.96 - ETA: 43:29 - loss: 0.0934 - acc: 0.96 - ETA: 41:34 - loss: 0.0932 - acc: 0.96 - ETA: 39:38 - loss: 0.0931 - acc: 0.96 - ETA: 37:43 - loss: 0.0930 - acc: 0.96 - ETA: 35:48 - loss: 0.0929 - acc: 0.96 - ETA: 33:52 - loss: 0.0927 - acc: 0.96 - ETA: 31:58 - loss: 0.0926 - acc: 0.96 - ETA: 30:02 - loss: 0.0925 - acc: 0.96 - ETA: 28:07 - loss: 0.0923 - acc: 0.96 - ETA: 26:11 - loss: 0.0922 - acc: 0.96 - ETA: 24:15 - loss: 0.0921 - acc: 0.96 - ETA: 22:19 - loss: 0.0920 - acc: 0.96 - ETA: 20:22 - loss: 0.0919 - acc: 0.96 - ETA: 18:26 - loss: 0.0918 - acc: 0.96 - ETA: 16:30 - loss: 0.0917 - acc: 0.96 - ETA: 14:34 - loss: 0.0917 - acc: 0.96 - ETA: 12:38 - loss: 0.0916 - acc: 0.96 - ETA: 10:42 - loss: 0.0914 - acc: 0.96 - ETA: 8:45 - loss: 0.0913 - acc: 0.9676 - ETA: 6:48 - loss: 0.0912 - acc: 0.967 - ETA: 4:51 - loss: 0.0911 - acc: 0.967 - ETA: 2:55 - loss: 0.0911 - acc: 0.967 - ETA: 57s - loss: 0.0910 - acc: 0.967 - 33269s 232ms/step - loss: 0.0909 - acc: 0.9678 - val_loss: 0.0625 - val_acc: 0.9771\n"
     ]
    }
   ],
   "source": [
    "modelLSTM.fit(X_train_seq, y_train, batch_size=batch_size, epochs=min(num_epochs,2), validation_split=0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - ETA: 1:37:5 - ETA: 1:41:4 - ETA: 1:42:3 - ETA: 1:40:5 - ETA: 1:40:1 - ETA: 1:41:0 - ETA: 1:40:3 - ETA: 1:39:3 - ETA: 1:38:3 - ETA: 1:38:0 - ETA: 1:37:0 - ETA: 1:36:1 - ETA: 1:35:2 - ETA: 1:34:5 - ETA: 1:34:0 - ETA: 1:33:2 - ETA: 1:32:3 - ETA: 1:31:5 - ETA: 1:31:1 - ETA: 1:30:2 - ETA: 1:29:4 - ETA: 1:28:5 - ETA: 1:27:5 - ETA: 1:26:5 - ETA: 1:26:0 - ETA: 1:25:0 - ETA: 1:24:1 - ETA: 1:23:2 - ETA: 1:22:2 - ETA: 1:21:3 - ETA: 1:20:4 - ETA: 1:20:0 - ETA: 1:19:1 - ETA: 1:18:3 - ETA: 1:17:5 - ETA: 1:17:2 - ETA: 1:16:5 - ETA: 1:16:2 - ETA: 1:15:4 - ETA: 1:15:1 - ETA: 1:14:4 - ETA: 1:14:0 - ETA: 1:13:3 - ETA: 1:12:5 - ETA: 1:12:2 - ETA: 1:11:4 - ETA: 1:11:0 - ETA: 1:10:3 - ETA: 1:09:5 - ETA: 1:09:1 - ETA: 1:08:4 - ETA: 1:08:0 - ETA: 1:07:2 - ETA: 1:06:4 - ETA: 1:06:0 - ETA: 1:05:2 - ETA: 1:04:4 - ETA: 1:03:5 - ETA: 1:03:1 - ETA: 1:02:3 - ETA: 1:02:0 - ETA: 1:01:3 - ETA: 1:01:0 - ETA: 1:00:2 - ETA: 59:49  - ETA: 59:0 - ETA: 58:2 - ETA: 57:3 - ETA: 56:5 - ETA: 56:0 - ETA: 55:2 - ETA: 54:4 - ETA: 53:5 - ETA: 53:1 - ETA: 52:3 - ETA: 51:5 - ETA: 51:2 - ETA: 50:5 - ETA: 50:1 - ETA: 49:3 - ETA: 48:5 - ETA: 48:1 - ETA: 47:4 - ETA: 46:5 - ETA: 46:1 - ETA: 45:3 - ETA: 44:5 - ETA: 44:2 - ETA: 43:4 - ETA: 43:0 - ETA: 42:2 - ETA: 41:3 - ETA: 40:5 - ETA: 40:1 - ETA: 39:3 - ETA: 38:5 - ETA: 38:1 - ETA: 37:3 - ETA: 36:4 - ETA: 36:0 - ETA: 35:2 - ETA: 34:3 - ETA: 33:5 - ETA: 33:1 - ETA: 32:2 - ETA: 31:4 - ETA: 31:0 - ETA: 30:2 - ETA: 29:3 - ETA: 28:5 - ETA: 28:0 - ETA: 27:2 - ETA: 26:3 - ETA: 25:5 - ETA: 25:0 - ETA: 24:2 - ETA: 23:4 - ETA: 22:5 - ETA: 22:1 - ETA: 21:2 - ETA: 20:4 - ETA: 20:0 - ETA: 19:1 - ETA: 18:3 - ETA: 17:4 - ETA: 17:0 - ETA: 16:2 - ETA: 15:3 - ETA: 14:5 - ETA: 14:1 - ETA: 13:2 - ETA: 12:4 - ETA: 12:0 - ETA: 11:2 - ETA: 10:3 - ETA: 9:5 - ETA: 9: - ETA: 8: - ETA: 7: - ETA: 6: - ETA: 6: - ETA: 5: - ETA: 4: - ETA: 4: - ETA: 3: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 25s - 6546s 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3710/153164 [00:09<06:40, 373.30it/s]C:\\Anaconda2\\envs\\py36\\lib\\site-packages\\tqdm\\_monitor.py:89: TqdmSynchronisationWarning: Set changed size during iteration (see https://github.com/tqdm/tqdm/issues/481)\n",
      "  TqdmSynchronisationWarning)\n",
      "100%|██████████| 153164/153164 [48:29<00:00, 52.64it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted = modelLSTM.predict(X_test_seq, batch_size=1024, verbose=1)\n",
    "columns = [\"id\",\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "dfTestPredicted = pd.DataFrame(columns=columns)\n",
    "for x in tqdm(range(len(test))):\n",
    "    dfTestPredicted.loc[x] = [test['id'][x], predicted[x][1], predicted[x][2], predicted[x][3], predicted[x][4], predicted[x][5], predicted[x][6]]\n",
    "dfTestPredicted.to_csv('../reports/testPred/predTestLSTM_Seq_'+ str(maxFeatures) +'.csv',encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfTestPredicted.to_csv('../reports/testPred/predTestLSTM_Seq_'+ str(maxFeatures) +'.csv',encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Word 2 Vec mean train list\n",
    "modelWord2VecTrain = models.Word2Vec(train['listOfCleanWords'].tolist(), min_count=1)\n",
    "#say_vector = modelWord2VecTrain['explanation']  # get vector for word\n",
    "#say_vector\n",
    "\n",
    "vectorizedTrainW2V = np.empty(len(train['listOfCleanWords']), dtype=list)\n",
    "for i, lstOfCleanWords in enumerate(train['listOfCleanWords']): \n",
    "    size = len(lstOfCleanWords)\n",
    "    if size > 0:\n",
    "        vector = np.zeros(len(modelWord2VecTrain[lstOfCleanWords[0]]))\n",
    "        for x in range(size):\n",
    "            vector += modelWord2VecTrain[lstOfCleanWords[x]]\n",
    "        vectorizedTrainW2V[i] = vector / size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
