{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Clasificación por Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### En este notebook, se procederá al uso de algoritmos esclusivamente de Deep Learning, mediante los datos que han sido previamente limpiados por \"Clean Words\". En este encontraremos en primer lugar, la utilización de Cross Validation entre los datos de entrenamiento, y posteriormente se realizará una prediccion sobre los datos de Test. Para poder ser evaluados por Kaggle.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import ast\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "# Linear Models \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, LSTM, Input, RNN\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.utils import plot_model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameTrainCSV = 'trainWithListOfCleanWords'\n",
    "nameTestCSV = 'testWithListOfCleanWords'\n",
    "\n",
    "train = pd.read_csv('../data/processed/' + nameTrainCSV + '.csv', encoding='utf-8')\n",
    "train['BagOfWords'] = dict\n",
    "train.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for x in tqdm(range(len(train))):\n",
    "    train.set_value(col='listOfCleanWords',\n",
    "                index=x,\n",
    "                value=ast.literal_eval(train[\"listOfCleanWords\"][x]))\n",
    "    train.set_value(col='cleanWordsAsText',\n",
    "                index=x,\n",
    "                value=str(train[\"cleanWordsAsText\"][x]))\n",
    "    train.set_value(col='BagOfWords',\n",
    "                index=x,\n",
    "                value=Counter(train[\"listOfCleanWords\"][x]))\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MULTICLASS PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classLabel = {\n",
    "    0: \"neutral\",\n",
    "    1: \"toxic\",\n",
    "    2 : \"severe_toxic\",\n",
    "    3 : \"obscene\",\n",
    "    4 : \"threat\",\n",
    "    5 : \"insult\",\n",
    "    6 : \"identity_hate\" \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.empty((len(train['cleanWordsAsText']),),dtype=object)\n",
    "allTextToxicTrain = dict()\n",
    "for idx in classLabel:\n",
    "    if classLabel[idx] != \"neutral\":\n",
    "        T = np.where(train[classLabel[idx]] == 1)[0]\n",
    "        allTextToxicTrain[idx] = T\n",
    "        for i in T:\n",
    "            if y[i] is None:\n",
    "                y[i] = [idx]                \n",
    "            else:\n",
    "                y[i].append(idx)\n",
    "indxsOfNeutralTexts = np.where(y == None) \n",
    "y[indxsOfNeutralTexts] = [[0]]\n",
    "indxsOfNeutralTexts = indxsOfNeutralTexts[0]\n",
    "\n",
    "allTextsNoToxicTrain = [str(train['cleanWordsAsText'][x]) for x in indxsOfNeutralTexts]\n",
    "\n",
    "idxList = []\n",
    "for i in allTextToxicTrain.keys():\n",
    "    #allTextToxicTrain[i] = [str(train['cleanWordsAsText'][j]) for j in allTextToxicTrain[i]]\n",
    "    idxList = np.unique(np.append(idxList, allTextToxicTrain[i]))\n",
    "allTextToxicTrain = [str(train['cleanWordsAsText'][j]) for j in idxList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test clasification\n",
    "test = pd.read_csv('../data/processed/' + nameTestCSV + '.csv', encoding='utf-8')\n",
    "test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Se inicializan las variables de X_train y X_test + Y_train completos a partir de los textos ya limpios, ademas de obtener todos los textos en forma de lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allTrainText = [txt if txt is not np.nan else '' for txt in train['cleanWordsAsText']]\n",
    "allTestText = [txt if txt is not np.nan else '' for txt in test['cleanWordsAsText']]\n",
    "X_train = allTrainText\n",
    "X_test = allTestText\n",
    "yBinary = MultiLabelBinarizer().fit_transform(y)\n",
    "y_train = yBinary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = [\"idExp\",\"numFeatures\", \"algorithm\", \"Nfolds\", \"accuaracy\", \"logloss\", \"fmeasure\"]\n",
    "dfTestResults = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Cantidad features a utilizar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxFeatures = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedings\n",
    "> En este apartado se diseñan 3 formas de representación de las palabras de los textos, utilizando directamente los textos ya limpiados previamente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mediante Tokenización de las palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "\n",
    "tokenizer = Tokenizer(num_words=maxFeatures)\n",
    "tokenizer.fit_on_texts(list(allTrainText))\n",
    "X_train_tokenized_seq = tokenizer.texts_to_sequences(allTrainText)\n",
    "X_test_tokenized_seq = tokenizer.texts_to_sequences(allTestText)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_seq = pad_sequences(X_train_tokenized_seq)\n",
    "X_test_seq = pad_sequences(X_test_tokenized_seq, maxlen=len(X_train_seq[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Selección de features a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train_seq\n",
    "X_test = X_test_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del modelo CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL CNN\n",
    "numClases = 7\n",
    "\n",
    "#training params\n",
    "batch_size = 512 \n",
    "num_epochs = 8 \n",
    "\n",
    "#model parameters\n",
    "num_filters = 128 \n",
    "weight_decay = 1e-4\n",
    "outputDim = 100\n",
    "\n",
    "modelCNN = Sequential()\n",
    "modelCNN.add(Embedding(input_dim=len(X_train), output_dim=outputDim))\n",
    "modelCNN.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "modelCNN.add(MaxPooling1D(2))\n",
    "modelCNN.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "modelCNN.add(GlobalMaxPooling1D())\n",
    "modelCNN.add(Dropout(0.5))\n",
    "modelCNN.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "modelCNN.add(Dense(numClases, activation='sigmoid'))  #multi-label (k-hot encoding)\n",
    "\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "modelCNN.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "modelCNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CV experiments\n",
    "idExp = 0\n",
    "numFeatures = maxFeatures\n",
    "\n",
    "# Cross validation\n",
    "Nfolds = 2\n",
    "kf = KFold(n_splits=Nfolds, random_state=True)\n",
    "kf.get_n_splits(X_train)\n",
    "\n",
    "name = \"CNN\"\n",
    "meanAcc = 0.0\n",
    "meanLogLoss = 0.0\n",
    "meanFmeasure = 0.0\n",
    "\n",
    "batch_size = 64 \n",
    "num_epochs = 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for train_index, test_index in tqdm(kf.split(X_train)):\n",
    "    X_train_cv = X_train[train_index]\n",
    "    X_test_cv = X_train[test_index]\n",
    "    y_train_cv, y_test_cv = yBinary[train_index], yBinary[test_index]\n",
    "\n",
    "    cnnmModelHist = modelCNN.fit(X_train_cv, y_train_cv, batch_size=batch_size, epochs=num_epochs, validation_split=0.1, shuffle=True, verbose=2)\n",
    "    predicted = modelCNN.predict(X_test_cv)\n",
    "\n",
    "    acc = accuracy_score(y_test_cv, predicted.round())\n",
    "    fmeausre = f1_score(y_test_cv, predicted.round(), labels=[0,1,2,3,4,5,6], average=None)\n",
    "    logloss = log_loss(y_pred=predicted.round(), y_true=y_test_cv)\n",
    "    meanAcc += acc\n",
    "    meanLogLoss += logloss\n",
    "    meanFmeasure += fmeausre\n",
    "meanAcc = meanAcc / Nfolds\n",
    "meanLogLoss = meanLogLoss / Nfolds\n",
    "meanFmeasure = meanFmeasure / Nfolds\n",
    "dfTestResults.loc[idExp] = [idExp,maxFeatures,name,Nfolds,meanAcc,meanLogLoss,meanFmeasure]\n",
    "print(str(idExp))\n",
    "idExp += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dfTestResults.to_excel('../reports/reportsCNN'+ str(maxFeatures) + '.xls', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción sobre test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> En este caso hacemos fitting mediante Tokenización, debido a que para clasificar textos a partir de CNN es la que mejor resultados da. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model training\n",
    "cnnmModelHist = modelCNN.fit(X_train, y_train, batch_size=batch_size, epochs=min(num_epochs, 3), validation_split=0.1, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = modelCNN.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Se almacenan los datos predecidos en formato CSV para poder hacer el submision de test, y poder evaluar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"id\",\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "dfTestPredicted = pd.DataFrame(columns=columns)\n",
    "for x in tqdm(range(len(test))):\n",
    "    dfTestPredicted.loc[x] = [test['id'][x], predicted[x][1], predicted[x][2], predicted[x][3], predicted[x][4], predicted[x][5], predicted[x][6]]\n",
    "dfTestPredicted.to_csv('../reports/testPred/predTestCNN_Seq_'+ str(maxFeatures) +'.csv',encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Creación del modelo LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(len(X_train_seq[0]), ))\n",
    "embed_size = 128\n",
    "x = Embedding(maxFeatures, embed_size)(inp)\n",
    "x = LSTM(90, return_sequences=True,name='lstm_layer')(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(60, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(7, activation=\"sigmoid\")(x)\n",
    "modelLSTM = Model(inputs=inp, outputs=x)\n",
    "modelLSTM.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfTestResults = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CV experiments\n",
    "idExp = 0\n",
    "X_train = X_train_seq\n",
    "X_test = X_test_seq\n",
    "numFeatures = maxFeatures\n",
    "\n",
    "# Cross validation\n",
    "Nfolds = 3\n",
    "kf = KFold(n_splits=Nfolds, random_state=True)\n",
    "kf.get_n_splits(X_train)\n",
    "\n",
    "name = \"LSTM\"\n",
    "meanAcc = 0.0\n",
    "meanLogLoss = 0.0\n",
    "meanFmeasure = 0.0\n",
    "\n",
    "batch_size = 64 \n",
    "num_epochs = 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for train_index, test_index in tqdm(kf.split(X_train)):\n",
    "    X_train_cv = X_train[train_index]\n",
    "    X_test_cv = X_train[test_index]\n",
    "    y_train_cv, y_test_cv = yBinary[train_index], yBinary[test_index]\n",
    "\n",
    "    cnnmModelHist = modelLSTM.fit(X_train_cv, y_train_cv, batch_size=batch_size, epochs=num_epochs, validation_split=0.1, shuffle=True, verbose=2)\n",
    "    predicted = modelLSTM.predict(X_test_cv)\n",
    "\n",
    "    acc = accuracy_score(y_test_cv, predicted.round())\n",
    "    fmeausre = f1_score(y_test_cv, predicted.round(), labels=[0,1,2,3,4,5,6], average=None)\n",
    "    logloss = log_loss(y_pred=predicted.round(), y_true=y_test_cv)\n",
    "    meanAcc += acc\n",
    "    meanLogLoss += logloss\n",
    "    meanFmeasure += fmeausre\n",
    "meanAcc = meanAcc / Nfolds\n",
    "meanLogLoss = meanLogLoss / Nfolds\n",
    "meanFmeasure = meanFmeasure / Nfolds\n",
    "dfTestResults.loc[idExp] = [idExp,maxFeatures,name,Nfolds,meanAcc,meanLogLoss,meanFmeasure]\n",
    "print(str(idExp))\n",
    "idExp += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfTestResults.to_excel('../reports/reportsCNN'+ str(maxFeatures) + '.xls', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción sobre test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso hacemos fitting mediante Tokenización, debido a que para clasificar textos a partir de LSTM es la que mejor resultados da."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLSTM.fit(X_train_seq, y_train, batch_size=batch_size, epochs=min(num_epochs,2), validation_split=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Se almacenan los datos predecidos en formato CSV para poder hacer el submision de test, y poder evaluar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = modelLSTM.predict(X_test_seq, batch_size=1024, verbose=1)\n",
    "columns = [\"id\",\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "dfTestPredicted = pd.DataFrame(columns=columns)\n",
    "for x in tqdm(range(len(test))):\n",
    "    dfTestPredicted.loc[x] = [test['id'][x], predicted[x][1], predicted[x][2], predicted[x][3], predicted[x][4], predicted[x][5], predicted[x][6]]\n",
    "dfTestPredicted.to_csv('../reports/testPred/predTestLSTM_Seq_'+ str(maxFeatures) +'.csv',encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
